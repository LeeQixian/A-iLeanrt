
## 七、 词汇的自动处理 (Lexical Processing)

### 1. 引言：词汇与现实世界的关联
老师引用了畅销书 **《魔鬼经济学》 (Freakonomics, 2005)** 中的案例，说明词汇统计如何揭示现实规律。

### 2. 案例分析：房地产广告用词与房价的关系
*   **研究对象**：房地产经纪人在广告中使用的形容词 vs 房屋最终售价。
*   **发现规律**：
    *   使用 **模糊/褒义形容词** (如 *Fantastic, Cute, Charming*)：
        *   往往暗示房屋有缺陷，试图掩盖。
        *   **统计结果**：这类词用得越多，最终售价反而**越低**。
    *   使用 **具体/物理描述词** (如 *Granite* 花岗岩, *Maple* 枫木)：
        *   传达具体信息。
        *   **统计结果**：这类词出现，售价往往**更高**。

### 3. 数学模型：线性回归 (Linear Regression)
通过统计方法将语言现象数学化。

*   **简单线性回归**：
    *   公式：$Y = mX + b$
    *   $Y$ = 房屋售价
    *   $X$ = 模糊形容词的数量
    *   $m$ = 斜率 (Slope)，统计得出为 **-4900** (即每多一个模糊词，售价约降低4900美元)。
    *   $b$ = 截距 (Intercept)，基础价格。
    *   **结论**：吹牛（使用模糊褒义词）对卖房不仅无益，反而与低价相关。

*   **多元线性回归 (Multivariate Linear Regression)**：
    *   现实中价格受多种因素影响，需引入特征向量 (Feature Vector)。
    *   公式：$Y = W_0 + W_1 F_1 + W_2 F_2 + W_3 F_3 + \dots$
    *   **特征 ($F$)**：
        *   $F_1$: 模糊形容词数量
        *   $F_2$: 市场未售房屋存量
        *   $F_3$: 房屋抵押率
    *   **权重 ($W$)**：通过数据训练得出每个特征对价格的影响权重 (Weight)。
        *   例如：$W_1$ 可能为 -5000，$W_2$ 可能为 -3000。

### 4. 总结
这个例子展示了计算语言学中**词汇研究**的一个重要侧面：通过统计词汇特征（Features）并建立数学模型（Regression），可以预测或解释现实世界的变量。这是自然语言处理中特征工程和统计学习的基础概念。


以下是根据课堂录音文稿（第二十一、二十二部分）整理的计算语言学课程知识要点笔记。此部分重点讲解了**词汇语义学（Lexical Semantics）**的基础理论、词汇关系以及词义消歧（WSD）的初步概念。

---

# 计算语言学（Lexical Semantics & Word Sense Disambiguation）课程笔记

## 一、 词汇研究的重要性与历史沿革
老师指出，当前计算语言学出现了一种**“词汇主义（Lexicalism）”**倾向，词汇处理是自然语言处理（NLP）的基础，不再仅仅被视为单词的堆砌。

### 1. 早期探索与重要学者
*   **De Morgan (1851, 英国)**：数学家。提出将**词长（Word Length）**作为文章风格的特征进行研究（例如某些作家偏爱长词）。
*   **Zipf (1935)**：提出**齐夫定律（Zipf's Law）**。发现词的**频度（Frequency）**与**序号（Rank，按频度高低排序）**之间成反比关系（乘积为常数）。这是语言学中重要的统计规律。
*   **Maurice Gross (莫里斯·格罗斯, 法国)**：
    *   **贡献**：建立了**词汇语法（Lexicon-Grammar）**。
    *   **工作**：对法语动词进行了系统研究，构建了19张矩阵表描述动词性质。他认为语言的形式知识主要来自词典，因此自动处理首先要查阅详细的电子词典。
    *   **背景**：乔姆斯基（Chomsky）的学生，工程背景出身，后转向词汇实证研究。
*   **Miller & Beckwith (普林斯顿大学)**：开发了 **WordNet**（英语词网），以**同义词集（Synset）**为基本单位组织词汇，是目前最重要的计算词汇资源之一。
*   **Katz & Fodor (1963)**：提出了**选择限制（Selectional Restriction）**概念。
    *   *案例*：动词“eat”的主语通常必须是“有生命的（Animate）”，宾语必须是“食品”。利用此规则可处理常识问题。
*   **Wilks (1975)**：提出了**优选语义学（Preference Semantics）**，利用选择限制对词义进行优选。
*   **Pustejovsky**：提出了**生成词库理论（Generative Lexicon）**，认为词典是动态的。

### 2. 国内的词汇资源建设
*   **北京大学计算语言学研究所**：建立了汉语语法信息词典（GKB），包含丰富的词性标注和语法属性。
*   **董振东**：建立了 **HowNet（知网）**，不仅研究单词，还研究词与词之间的概念关系（Concept definitions）。

## 二、 词汇语义学基础理论

### 1. 弗雷格的主成性原理 (Compositionality Principle)
*   **提出者**：Frege (德国哲学家/逻辑学家)。
*   **核心观点**：**句子的意思是由构成该句子的单词的意思以及这些单词之间的语法结构关系组成的。**
*   **计算语言学应用**：做机器翻译或NLP系统时，首先通过词典获取单词信息（内因），再结合语法规则（句法分析）推导句子含义。
    *   *口诀*：词典 + 规则（句法+语义）= 系统基础。

### 2. 基本概念定义
*   **词汇 (Lexicon)**：不仅仅是单词的无序堆砌，而是一个**高度系统化的结构**。
*   **词 (Word) vs. 词位 (Lexeme)**：
    *   由于“Word”定义模糊，计算语言学倾向使用 **Lexeme**。
    *   **定义**：词典中的一个单独条目，是特定的拼写形式、语音形式与意义表示形式的组合。
*   **义项 (Sense)**：一个词汇的具体含义。

## 三、 词汇关系 (Lexical Relations)

### 1. 同形/同音关系 (Homonymy)
*   **定义**：形式相同但意义上**没有联系**（来源不同）的词汇关系。
*   **案例 - Bank**：
    *   含义A：银行（源自意大利语 *banca*）。
    *   含义B：河岸（源自斯堪的纳维亚语）。
    *   两者拼写相同，但意义毫无关联。
*   **分类**：
    *   **同音词 (Homophones)**：读音相同，拼写不同（如 *two, to, too*）。
    *   **同形词 (Homographs)**：拼写相同，读音可能不同（如 *bass* 乐器 vs. *bass* 鱼）。

### 2. 多义关系 (Polysemy)
*   **定义**：一个词具有多个**彼此相关**（有引申、缩小等联系）的意义。
*   **案例 - Head**：
    *   含义1：人头（解剖学）。
    *   含义2：物体的顶部/前哨（如 *head of the bed*, *ship head*）。
    *   含义3：头脑/智力（如 *get facts into your head*）。
    *   *关系*：由实体引申为非实体，或由整体缩小为局部。

### 3. 如何区分 Homonymy (同形) 与 Polysemy (多义)？
老师介绍了两种判断方法：
*   **方法一：词源判断法 (Etymology)**
    *   看词源是否相同。来源相同为多义，来源不同为同形。
    *   *缺点*：需要深厚的语言学考证，对计算机难操作。
*   **方法二：共现搭配法 (Zeugma / Trigger Criteria)**
    *   **操作**：用 `and` 将该词的两个宾语/搭配连起来，看句子是否自然。
    *   **案例 A (Flight)**：
        *   *Which flights serve breakfast?* (serve = 供应食品)
        *   *Does Air France serve Philadelphia?* (serve = 提供服务/飞往)
        *   *测试*：*Does Air France serve breakfast and Philadelphia?* -> **句子很怪**。
        *   *结论*：含义距离远，算作不同义项（或同形）。
    *   **案例 B (Play)**：
        *   *Play soccer* (体育运动) & *Play basketball* (体育运动) -> *Play soccer and basketball.* (通顺，含义一致)。
        *   *Play soccer* & *Play piano* (演奏乐器) -> *Play soccer and piano.* (**很怪**)。
        *   *Play soccer* & *Play doctor* (扮演角色) -> (**很怪**)。
    *   *结论*：若搭配后句子可接受度差，说明是不同的义项。这是计算语言学常用的有效判断手段。

### 4. 计算机视角的处理
*   **观点**：对于计算机而言，**Homonymy（同形）和 Polysemy（多义）没有本质区别**。
*   **核心问题**：都需要解决**歧义（Ambiguity）**问题，即 Word Sense Disambiguation (WSD)。只要能区分开即可，不用像语言学家那样纠结词源。

### 5. 同义关系 (Synonymy)
*   **定义**：不同词汇具有相同意义。
*   **判定标准**：**可替换性 (Substitutability)**。若替换后句子意思和可接受度不变，则为同义词。
*   **影响替换的四个因素**：
    1.  **多义性**：在某个义项上是同义词，在另一个义项上可能不是。（如 *big* 和 *large* 在 *plane* 上可互换，但在 *sister* 上，*big sister* 指姐姐，*large sister* 指体型大）。
    2.  **搭配约束 (Collocation)**：习惯用法。
        *   *Strong tea* (浓茶) - 对。 *Powerful tea* - 错。
        *   *Powerful whiskey* - 错。
    3.  **情感色彩**：褒义、贬义、中性。
    4.  **语体/领域 (Register)**：
        *   *Father/Mother* (正式) vs. *Dad/Mom* (非正式)。
        *   *Social status* 相关用词。

### 6. 上下位关系 (Hyponymy/Hypernymy)
*   **定义**：
    *   **Hypernym (上位词)**：一般概念（如 *Vehicle*）。
    *   **Hyponym (下位词)**：具体概念（如 *Car*）。
*   **逻辑蕴含**：*There is a car* implies *There is a vehicle*.
*   **本体论与分类体系 (Ontology)**：
    *   **亚里士多德**：早在2000多年前将世界万物分为10大类（范畴），这种分类思想非常了不起。
    *   **机器翻译应用**：老师自建的翻译系统将词汇分为16类（实体、事件、属性、时间、空间等），并进一步细化，形成概念层级（Hierarchy）。
    *   相关资源：WordNet, FrameNet (Fillmore).

## 四、 词义消歧 (Word Sense Disambiguation, WSD)

### 1. 歧义现象的普遍性
一词多义是语言的普遍现象，若不解决会导致机器翻译严重错误。

### 2. 经典案例： "May I help you?"
*   **机器翻译错误**：曾被翻译成“五月我帮你”（May = 五月）。
*   **原因**：词典中 *May* (大写) 对应“五月”，*may* (小写) 对应“可以”。句首必须大写，导致机器误判。
*   **修正**：增加规则——位于句首的 *May* 既可能是月份也可能是情态动词，需根据上下文判断。

### 3. 歧义类型
*   **名词歧义**：
    *   *Bachelor*：单身汉 vs. 学士学位。
        *   *John is a bachelor.* (模糊)
        *   *John is a medical doctor.* vs. *John is a PhD.* (Doctor: 医生 vs. 博士)
    *   *Glass*：玻璃杯 vs. 眼镜 (*glasses*).
*   **同形歧义**：
    *   *Bank*：河岸 vs. 银行。
    *   *Rest*：休息 vs. 剩余部分 (*The rest of the army was insufficient*).

### 4. 消歧方法简述
*   **基于词典 (Dictionary-based)**。
*   **有指导的机器学习 (Supervised Learning)**：利用标注好的语料库训练。
*   **无指导的机器学习 (Unsupervised Learning)**：利用生语料聚类。
*   **基于规则**：利用上下文搭配、选择限制等。
以下是根据课堂录音文稿（二十三、二十四）整理的计算语言学课程知识要点笔记：

# 计算语言学课程笔记（续）

## 五、 语言中的歧义现象（Ambiguity）
歧义是自然语言处理中非常普遍且困难的问题，存在于语言的各个层面。

### 1. 结构与指代歧义（Structural & Referential Ambiguity）
*   **单复数引起的歧义**：
    *   例：`The sheep grass in the field.` （*sheep* 单复数同形，不确定是一只羊还是一群羊）。
    *   例：`The condemned person` vs `The condemned`（形容词名词化后，不知是单数还是复数）。
*   **缩写引起的歧义**：
    *   例：`ABC` 可能指 `Australian Broadcasting Company` 或 `American Broadcasting Company`。
    *   例：`WWW` 可能指 `World Wide Web` 或 `World With War`。
*   **指代不明（Pronoun Reference）**：
    *   例：`Nobody said he was wrong.` （*he* 是指说话人自己，还是第三者？）
    *   例：`He killed himself` vs `He shot himself`.
        *   `killed himself`：明确是自杀。
        *   `shot himself`：可能是自杀，也可能是擦枪走火打伤自己。
*   **量词/范围歧义（Scope Ambiguity）**：
    *   例：`Everyone was eating a large cake.`
        *   理解A：大家共同吃同一个大蛋糕（One cake for all）。
        *   理解B：每个人各自吃一个大蛋糕（Many cakes）。
    *   例：`Every child loves a girl.`
        *   理解A：所有孩子都喜欢同一个特定的女孩。
        *   理解B：每个孩子都有自己喜欢的女孩（分别对应）。

### 2. 词汇歧义（Lexical Ambiguity）
当一个词存在多个义项时，机器难以判断具体含义。
*   **名词歧义**：
    *   `Bachelor`：学士 vs 单身汉。
    *   `Bank`：河岸 vs 银行（同形异义词 Homonymy）。
    *   `Glasses`：眼镜 vs 玻璃杯。
    *   `Pen`：钢笔 vs 围栏（Bar-Hillel 提出的经典案例）。
*   **动词歧义**：
    *   `Draw/Pull`：
        *   `John is pulling a cart.`（拉车）
        *   `John is drawing a car.`（画车 vs 拉车）。
    *   `Saw`：
        *   `They never saw the wood.`（没看见木头 - *see* 的过去式）。
        *   `They never saw the wood.`（没锯木头 - *saw* 为锯子/锯）。
    *   `Make/Fast`：`make the ship fast`（使船加速 vs 把船系紧/固定）。
*   **形容词歧义**：
    *   `Poor`：`A poor mechanic`（没钱的技工 vs 技术差的技工）。
    *   `Japanese`：`Student of Japanese`（学日语的学生）vs `Japanese student`（日本籍学生）。
    *   `Sweet`：`A sweet salesman`（和蔼的推销员 vs 卖糖果的人）。
*   **连接词与介词歧义**：
    *   `As`：时间（当...时）vs 原因（因为）。
    *   `While`：时间（当...时）vs 让步（尽管）。
    *   `Since`：时间（自从）vs 原因（因为）。
    *   `Of`：`Reminiscence of my father`（父亲写的回忆录 vs 关于父亲的回忆录）。
    *   `With`：伴随（在一起）vs 工具（用...）。

---

## 六、 词义排歧（Word Sense Disambiguation, WSD）

### 1. 术语辨析
*   **术语**：Word Sense Disambiguation (WSD) 或 Word Sense Resolution。
*   **观点**：老师建议使用**“排歧”**而非“消歧”。
    *   原因：“消歧”在口语中容易被误解为“消气”（生气后平复）或“消息”，外行听不懂；“排歧”更准确地表达了“排除歧义”的过程。

### 2. WSD 的困难性与重要性
*   **困难性**：
    *   歧义具有很强的**特异性**（Specificity）。每个词的歧义触发条件不同，难以总结出通用的句法规则。
    *   例如：`Bank` 的消歧依赖 `river` 或 `money` 等上下文词汇；而 `Face` 的消歧依赖其他词汇。若用规则法，需要为每个词单独写规则，工作量巨大且难以穷尽。
*   **历史背景**：
    *   **Bar-Hillel (1959)** 的论断：全自动高质量机器翻译（FAHQMT）是不可能的。
    *   **经典反例**：`The box was in the pen.`（盒子在围栏里）。
        *   机器倾向于将 `pen` 翻译为“钢笔”。
        *   要正确理解为“围栏”，需要常识（World Knowledge）：小孩玩耍时会待在围栏（playpen）里。当时的计算机无法处理这种常识。
*   **重要性**：
    *   2007年 ACL 会议论文指出：WSD 能显著提高统计机器翻译（SMT）的质量。

### 3. WSD 的主要方法
老师总结了解决词义排歧的三种主要途径：
1.  **基于规则的方法 (Rule-based)**：编写特定规则（困难，不可扩展）。
2.  **基于统计/机器学习的方法 (Machine Learning)**：
    *   利用语料库（Corpus）让机器自动学习排歧知识。
    *   分为：有监督（Supervised）、半监督（Semi-supervised）、无监督（Unsupervised）。
3.  **基于知识源的方法 (Knowledge-based)**：利用现有的电子词典、WordNet 等知识库进行排歧。

---

## 七、 具体的排歧策略：第一种方法

### 方法一：选择最常见的含义 (Most Frequent Sense, MFS)
*   **原理**：
    *   这是一种“偷懒”但实用的基准方法。
    *   不进行复杂的上下文分析，总是选择该词在词典或语料统计中出现频率最高的那个义项（Default sense）。
*   **案例分析**：
    *   句子：`Pupils from a school in north Beijing met with a film star.`
    *   歧义词处理：
        *   `Pupil`：常见义为“学生”（忽略“瞳孔”）。
        *   `School`：常见义为“学校”（忽略“鱼群”）。
        *   `Film`：常见义为“电影”（忽略“胶卷/薄膜”）。
        *   `Star`：常见义为“明星”（忽略“恒星”）。
    *   结果：通过取最常见义项，整个句子的理解是正确的。
*   **局限性与准确率**：
    *   **准确率**：在通用英语文本中，仅靠MFS方法大约能达到 **60%-70%** 的正确率（亦称为 Baseline）。
    *   **问题**：对于剩下的30%-40%，该方法会产生严重的错误或笑话（如将行李很轻 `light` 翻译为行李很亮）。
    *   **应用**：早期的机器翻译系统常采用此法，将最常见义项放词典第一位，实际上忽略了后续义项。

### 方法二：利用词类排歧 (Part-of-Speech based disambiguation)
（注：此部分刚开始提及）
*   **思路**：如果一个词的不同义项对应不同的词性（Part-of-Speech），则可以通过词性标注来解决歧义。
*   这是基于规则的一种简单形式。

## 一、 基于词类的词义排歧（Disambiguation via Part-of-Speech）

利用词类（Part-of-Speech, POS）来区分词义是一种常见且有效的方法。如果一个词在作名词和动词时具有完全不同的含义，通过判断其词性即可确定其意义。

### 1. 案例分析
*   **Face**:
    *   **动词**：面对。例："The house **faces** the park."（前面有名词词组，后面有名词词组，判断为动词）。
    *   **名词**：面孔。例："She pulled a long **face**."（前面有冠词/形容词，判断为名词）。
*   **May**:
    *   **名词**：五月（通常首字母大写）。例："**May Day** is the first day of May."
    *   **助动词**：可以/可能（通常小写，句首除外）。例："**May** I help you?"
    *   **规则判定**：句首 + 代词（I/You）+ 动词原形 -> 判定为助动词（可以）。
*   **Can**:
    *   **助动词**：能够（德语 *können*）。例："She **can** speak German."（代词 + can + 动词 -> 助动词）。
    *   **名词**：罐头。例："He opened a **tin/can** of beans."（不定冠词 + can + of -> 名词）。
*   **Will**（录音中误识别为Where/Wear）:
    *   **助动词**：将要。例："It **will** rain tomorrow."（代词 + will + 动词）。
    *   **名词**：意志。例："Free **will** makes us able to choose..."（形容词 free 修饰 -> 名词）。
*   **Kind**:
    *   **名词**：种类。例："That **kind** of book."（指示代词后，of前）。
    *   **形容词**：亲切/和蔼。例："It was very **kind** of you."（very修饰 -> 形容词）。

### 2. 方法总结与局限性
*   **方法**：编写规则（Rule-based）。根据上下文（如前后的冠词、介词、代词）确定词性，进而确定词义。
*   **局限性**：
    1.  **工作量巨大**：编写规则非常繁琐。老师提到曾指导一名韩国博士生，花费两年多时间仅解决了5个常用代词的指代消解问题。规则数量多且相互作用复杂。
    2.  **同词性多义（Polysemy within POS）**：即使确定了词性，词义仍可能存在歧义。
        *   例 **Work/Works**（名词）：
            *   "Gas **works**" -> 煤气厂（工厂）。
            *   "Shakespeare's **works**" -> 莎士比亚著作（作品）。
            *   "My daughter **works**" -> 工作（动词，较易区分）。
            *   仅靠词性无法区分“工厂”和“著作”，需要更复杂的语义规则。

---

## 二、 基于选择限制的词义排歧（Selectional Restriction）

当词性本身不足以排歧时，需要利用词与词之间的语义搭配约束，即**选择限制（Selectional Restriction）**。

### 1. 理论背景
*   **提出者**：Katz & Fodor (1963)。他们属于乔姆斯基（Chomsky）学派，将语义规则引入标准理论。
*   **核心思想**：动词或形容词对其支配的名词（主语、宾语等）有特定的语义类别要求。

### 2. 案例分析：Handsome
*   **Handsome** 的三个义项及其选择限制：
    1.  **漂亮的/英俊的**：搭配对象需为 **Human**（人）或 **Artifact**（人工制品）。
        *   例：Handsome fellow（人），Handsome building（建筑）。
    2.  **慷慨的**：搭配对象需为 **Conduct**（行为）。
        *   例：Handsome treatment（慷慨的待遇）。
    3.  **可观的（数量大）**：搭配对象需为 **Amount**（数量）。
        *   例：Handsome sum（可观的一笔钱）。

### 3. 工具资源：同义词词典（Thesaurus）
*   利用带有语义分类代码的词典（如梅家驹编纂的《同义词词林》）给文本中的词标注语义类别（Semantic Class/Code）。
*   通过匹配上下文词汇的语义代码与动词/形容词的选择限制，实现自动排歧。
*   **观点**：从事计算语言学需要扎实的语言学基础和外语能力，否则编写的规则质量会很差。

---

## 三、 Hirst 的语义消歧方法

Graeme Hirst 进一步发展了利用上下文关系进行排歧的方法，核心理念是 **"You shall know a word by the company it keeps"**（观其伴而知其意）。

### 1. 案例：Dishes
*   **语境A**："washing dishes"
    *   **dish** 含义：盘子/碟子（Physical Object）。
    *   **依据**：动词 *wash* 要求宾语具有 `washable`（可清洗）属性。
*   **语境B**："stir-frying dishes"
    *   **dish** 含义：菜肴（Food）。
    *   **依据**：动词 *stir-fry*（炒）要求宾语具有 `edible`（可食用）属性。

### 2. 案例：Crane（起重机 vs 鹤）
*   **语境A**："The crane flew over the plain."
    *   **线索**：动词 *flew*（飞）。
    *   **限制**：*Fly* 的主语必须是 **Bird**。
    *   **结论**：Crane = 鹤。
*   **语境B**："The builder operated the crane."
    *   **线索**：动词 *operate*（操作）。
    *   **限制**：*Operate* 的宾语通常是 **Machine**。
    *   **结论**：Crane = 起重机（机器）。

---

## 四、 语义距离计算（Semantic Distance Calculation）

Hirst 提出通过计算概念间的语义距离来确定词义。建立语义框架（Semantic Frame）或本体网络。

### 1. 语义网络结构
*   **节点**：概念（如 Crane1, Crane2, Bird, Machine, Wing）。
*   **关系**：
    *   **Is-A** (Instance of/Kind of)：上下位关系（如 Bird is an Animal）。
    *   **Has-Part**：整体-部分关系（如 Bird has part Wing）。

### 2. 边的权重（Weighting）与方向
Hirst 给不同方向的边赋予不同的权重（表示语义距离）：
*   **Is-A 正向**（向上，特指到一般）：距离小，权重 **0.1**（因为属于常识，联系紧密）。
*   **Is-A 反向**（向下，一般到特指）：距离大，权重 **0.95**（因为子类众多，不确定性大）。
*   **Has-Part 正向**：权重 **0.3**。
*   **Has-Part 反向**：权重 **0.8**。

### 3. 计算逻辑与公式
*   **目标**：判断词 A（如 wing）与多义词 B（如 crane）的哪个义项更接近。
*   **公式**：
    *   计算 A 到 B 的路径距离总和：$Distance(A \to B)$
    *   计算 B 到 A 的路径距离总和：$Distance(B \to A)$
    *   取两者中的**最小值**作为最终语义距离。
    *   $SemanticDistance = Min(Dist(A \to B), Dist(B \to A))$

### 4. 计算演示：Wing（翅膀）属于 Crane 的哪个义项？
*   **候选义项**：
    *   **Crane 1**：鹤（Bird）。
    *   **Crane 2**：起重机（Machine）。
*   **路径 1：Wing 与 Crane 1 (Bird)**
    *   路径：Wing -> (part of) -> Bird -> (is a) -> Crane 1 (注意：此处录音逻辑为反推，即 Crane 1 is a Bird, Bird has part Wing)。
    *   根据录音中的示例计算：
        *   Win -> Crane 1 距离：1.75
        *   Crane 1 -> Win 距离：**0.4** (Crane1 --(is a 0.1)--> Bird --(has part 0.3)--> Wing)
        *   **最小距离**：0.4
*   **路径 2：Wing 与 Crane 2 (Machine)**
    *   路径涉及：Crane 2 is a Machine, Machine is a Physical Object... Wing is a Physical Object. 路径很长且需经过顶层节点。
    *   根据录音中的示例计算：
        *   Crane 2 -> Win 距离：1.15 (或 2.0，录音口述数值略有跳跃，但逻辑明确)
        *   **最小距离**：1.15
*   **结论**：
    *   因为 0.4 < 1.15，所以 **Wing** 与 **Crane 1 (鹤)** 的语义距离更近。
    *   系统自动判断：在此上下文中，Crane 的意思是“鹤”。

这是根据您提供的第27、28部分录音文稿整理的计算语言学课程知识要点笔记。

# 计算语言学课程笔记 (续)

## 六、 词义消歧方法：基于选择限制 (Selectional Restrictions)

### 1. 基本原理
利用谓词（动词）与其论元（名词）之间的语义选择限制关系来判断词义。
*   **双向判断**：
    *   通过动词的论元要求，判断动词的含义。
    *   反之，通过动词的语义，消除名词论元的歧义。

### 2. 案例分析：Serve 的歧义消除
动词 **Serve** 主要有三种含义：
1.  **进食/品尝 (Eat/Taste)**：宾语 (Patient) 是食物。
2.  **服务/飞往 (Fly to/Service)**：宾语是地点。
3.  **提供/供应 (Provide)**：宾语是餐食名称 (如 breakfast)。

*   **实例辨析**：
    *   *Serve green lip mussel*：宾语是 mussel（食物），故 serve 为“品尝”。
    *   *Which airline serves Denver?*：宾语 Denver 是地点，故 serve 为“飞往/服务”。
    *   *Which one serves breakfast?*：宾语 breakfast 是餐名，故 serve 为“供应”。

*   **复杂案例**：*I'm looking for a restaurant that serve vegetarian dishes.*
    *   **Dishes** 歧义：菜肴 vs. 盘子。
    *   **Serve** 歧义：上述三种。
    *   **推导**：Vegetarian（素食的）修饰 dishes，故 dishes 为“菜肴”（食品）。Serve 的宾语是食品，且主语是 restaurant，故 serve 为“提供/供应”。

### 3. 知识资源
实施该方法需要依托语义知识库：
*   **WordNet** (普林斯顿大学)
*   **HowNet** (知网)
*   **北大语义词典**

## 七、 选择限制方法的局限性与改进

### 1. Hirst (1987) 提出的四种局限情况
虽然选择限制有用，但在以下情况会失效：
1.  **一般性太强 (Too general)**：如 *What kind of dishes do you recommend?* (recommend 对物体无特殊限制，无法区分 dishes 是菜还是盘子)。
2.  **否定句 (Negation)**：如 *You cannot eat gold.* (Gold 不是食物，违反限制，但句子合法)。
3.  **异常事件 (Unusual events)**：如 *Mr. Cooking ate glass.* (吃玻璃违反常理，但语境中可能是特技表演，句子合法)。
4.  **隐喻与转喻 (Metaphor & Metonymy)**：如 *Soviet Union ate Afghanistan.* (国家不能吃东西，这是隐喻)。

### 2. 改进方案：优选语义 (Preference Semantics)
针对硬性规则太死板的问题，引入“优选”或“概率”概念。
*   **Wilks (1975) - 优选语义学 (Preferred Semantics)**：
    *   观点：不把限制看作绝对的接受/拒绝，而看作一种**偏好 (Preference)**。
    *   案例：*The policeman interrogated the crook.*
        *   Crook 歧义：骗子 vs. 牧羊杖。
        *   Interrogate（审问）的主宾语倾向于 Human。因此优选“骗子”，排除“牧羊杖”。
    *   意义：即使在隐喻或童话（如猫说话）中，也能通过降低优选度而非报错来处理。

*   **Resnik (1997) - 选择关联度 (Selectional Association)**：
    *   方法：将优选概念量化为**概率 (Probability)**。
    *   计算：结合句法分析器 (Parser) 和语义层级 (WordNet)，计算动词与特定语义类别的论元在语料库中共同出现的概率关联度。

*   **放宽限制条件**：
    *   案例：*The company agreed the proposal.*
    *   调整：将 agree 的主语限制从“人 (Human)”放宽到“社会实体 (Social Object)”，涵盖政府、公司等。

## 八、 词义消歧方法：基于语料库的自立方法 (Stand-alone Approach)

### 1. 方法论转变
*   **Rule-to-Rule (基于规则)**：依赖人工编写的语言学规则。
*   **Stand-alone / Robust (自立/鲁棒方法)**：不依赖人工规则，而是通过**机器学习 (Machine Learning)** 从语料库中自动获取知识。

### 2. 基本流程
1.  **确定目标**：Target Word (目标词) + Context (上下文窗口)。
2.  **预处理**：词性标注 (POS Tagging)、词形还原 (Lemmatization)、局部句法分析。
3.  **特征提取 (Feature Extraction)**：建立特征向量 (Feature Vector)。

### 3. 特征类型
*   **搭配特征 (Collocational Features)**：与目标词有固定位置关系的词（如前后1-2个词）。
    *   *Yarowsky 案例 (Bass)*：*...electric guitar and bass player stand...*
    *   特征：[guitar, and, (bass), player, stand]。
*   **共现特征 (Co-occurrence Features)**：在上下文中出现，但不一定紧邻，反映主题关联。
    *   *Yarowsky 案例*：提取 fishing, song, rod, band 等12个词构建向量（出现为1，未出现为0）。
    *   逻辑：若向量中 guitar, player 位为1，则 bass 指“低音乐器”；若 fishing, rod 位为1，则指“鲈鱼”。

### 4. Hanks 的改进 (Bank 案例)
*   将共现词人工分为语义组：
    *   A组 (Money/Financial): morning, note, check, investment -> Bank = 银行。
    *   B组 (River/Nature): river, swim, lake, boat -> Bank = 河岸。
*   **性质**：介于传统规则与纯统计之间的过渡方法。

## 九、 词义消歧方法：有指导的机器学习 (Supervised Learning)

### 1. 概述
*   利用已标注词义的语料库 (Sense-tagged corpus) 训练分类器。
*   主要方法：朴素贝叶斯、决策表等。

### 2. 朴素贝叶斯分类器 (Naive Bayes Classifier)
*   **核心思想**：在给定的上下文特征向量 ($V$) 下，计算各个候选词义 ($S$) 的概率，选择概率最大的那个词义。
*   **公式推导**：
    *   目标：寻找 $\hat{S} = \operatorname{argmax}_{S} P(S|V)$
    *   利用贝叶斯公式转换：$P(S|V) = \frac{P(V|S) \cdot P(S)}{P(V)}$
    *   由于 $P(V)$ 对所有词义是常数，仅需最大化分子：
        $$ \hat{S} = \operatorname{argmax}_{S \in Senses} P(S) \cdot P(V|S) $$
    *   $P(S)$：该词义的先验概率（在语料中出现的频率）。
    *   $P(V|S)$：在该词义下，出现特征向量 $V$ 的似然概率。


这是根据您提供的“计算语言学（三十）”和“计算语言学（二十九）”两份录音文稿整理的课程知识要点笔记。

**注意**：根据内容逻辑，这两份文稿紧密相关，主要涵盖了科研素养建议、词义消歧（WSD）的基于词典方法、以及基于机器学习（监督、半监督、无监督）的方法，最后是对学科发展的反思。

---

# 计算语言学课程笔记：词义消歧进阶与机器学习方法

## 一、 科研素养与基本能力培养（课前建议）

老师首先强调了博士生和研究人员应具备的基本素质，认为这些比表面的名气更重要。

### 1. 三种基本能力
*   **语言表达能力**：
    *   包括阅读（中文、外文）和写作。
    *   **翻译训练**：不应视为负担，而是心理和语言能力的训练。建议逐句翻译，琢磨上下文，以此提高中英文水平。
    *   观点：语言能力是人类最重要的能力之一，是文化的表现。
*   **数学逻辑能力**：不一定要求高深的数学技巧，但需要具备严谨的逻辑思维。
*   **科学思维能力**：进行科学研究、发现问题和解决问题的能力。

### 2. 学术态度
*   **硬实力（Hard Power）**：找工作和学术声誉依靠的是发表的高质量文章（Paper），而非导师的名气或媒体曝光度。
*   **脚踏实地**：要在科研过程中严格要求自己，不懂就问，老实做学问，不要被表面的浮华（如媒体采访）所迷惑。

---

## 二、 词义消歧（Word Sense Disambiguation, WSD）方法论

课程回顾了之前讲过的“最常见义项法”和“基于规则的方法”，重点展开了**基于词典**和**机器学习**的方法。

### 1. 利用已有知识源（基于词典）的方法
利用机器可读词典（Machine Readable Dictionaries, MRD）中的定义（Definition）来进行词义排歧。

*   **原理**：通过比较句子中词语的定义，寻找重合（Overlap）的词汇，从而判断词义。
*   **代表人物**：Lesk (1986)。
*   **案例分析**：
    *   **"Pine cone"（松果）**：
        *   *Pine* 的义项：1. Evergreen tree（常绿树）；2. Wasting away (sorrow/illness)。
        *   *Cone* 的义项：1. Solid body...; 2. ...; 3. Fruit of certain **evergreen trees**。
        *   **消歧**：因为两个词的定义中都出现了 "Evergreen tree"，因此判定 *Pine* 取树木义，*Cone* 取果实义。
    *   **"The box is in the *pen*" 与 "*Sheep*"**：
        *   *Pen* 的义项：1. Writing instrument; 2. Enclosure for **domestic animals**。
        *   *Sheep* 的义项：... **domestic animals** ...
        *   **消歧**：定义中 "domestic animals" 重合，因此 *Pen* 指“围栏”。
    *   **"Fish *with* a fork" vs "Fish *with* bones"（介词附着歧义）**：
        *   *With* 的义项：1. Instrument（工具）；2. Part of（部分）。
        *   *Fork* 定义中有 "Instrument" -> 此处 *With* 解读为“用”（工具）。
        *   *Bones* 定义中有 "Part of animal", *Fish* 定义中有 "Animal" -> 此处 *With* 解读为“带有”（部分）。
*   **优点**：利用现成知识（词典），无需编写大量规则，可处理大规模词汇。
*   **效果**：Lesk 实验正确率在 50% - 72% 之间。

---

## 三、 基于机器学习的词义消歧

随着发展，方法从“人工写规则”转向“让机器从语料库中学习”。

### 1. 有指导的学习方法 (Supervised Learning)
需要人工对语料进行标注（Annotated Corpus），机器从中学习特征。

*   **贝叶斯分类 (Naive Bayes)**：
    *   **原理**：计算在给定上下文特征（Feature向量）下，某词义出现的概率。利用贝叶斯公式，计算上下文与各义项的乘积，取最大值。
    *   **Gale et al. (1992) 实验**：对 *duty, drug, land, language, politician, conference* 6个多义词进行消歧，正确率达 90% 左右。
*   **决策表/决策列表 (Decision Lists)**：
    *   **代表人物**：Yarowsky (1996)。
    *   **原理**：建立一系列规则（Conditions），按对数似然比（Log-likelihood）排序。
    *   **案例 "Bass"（鲈鱼 vs 低音/乐器）**：
        *   规则示例：
            *   若窗口中出现 *fish* -> 判断为 *Bass1* (鱼)。
            *   若窗口中出现 *play* -> 判断为 *Bass2* (乐器)。
            *   若窗口中出现 *violin* -> 判断为 *Bass2* (乐器)。
            *   若窗口中出现 *river* -> 判断为 *Bass1* (鱼)。
    *   **特点**：虽然规则形式像人写的，但是是通过语料库训练得到的，依赖人工标注的数据。

### 2. 半指导的学习方法 (Semi-supervised / Bootstrapping)
又称“自举法”（Bootstrapping），老师特别提到这个词翻译很难，意为“自己提着靴子把自己拉起来”，即依靠极少量数据自我学习。

*   **核心思想**：
    *   **One sense per collocation**：一个特定的搭配通常对应一个特定的义项。
    *   不需要大量标注语料，只需要少量的**种子（Seeds）**。
*   **操作步骤**：
    *   人工选择极少量可靠的“种子”词（Indicator words）。
    *   用种子去语料库中检索，对部分数据进行分类。
    *   利用分类后的数据训练，扩大规模，再对新数据分类，循环往复（迭代）。
*   **案例 "Bass"**：
    *   种子选择：用 *fish* 代表“鱼”义项，用 *play* 代表“乐器”义项。
    *   **推导过程**：
        *   句子出现 "...play the free bass..." -> 含有种子 *play* -> 标记为乐器。
        *   句子出现 "...sea bass..." -> 含有种子 *fish* (或相关词) -> 标记为鱼。
        *   机器自动扩展：发现 *player* 常与 *play* 共现，于是 *player* 也成为判定为“乐器”的新特征。
*   **效果**：Yarowsky 对12个多义词实验，正确率达 96.5%。
*   **优点**：省去了大量人工标注的繁琐，利用了机器的计算能力。

### 3. 无指导的学习方法 (Unsupervised Learning)
完全不需要人工标注，仅依靠生语料（Raw Corpus）中的统计关系。

*   **原理**：**向量聚类 (Vector Clustering)**。
    *   基于上下文词语的共现频率（Co-occurrence）。
    *   如果两个词义不同，它们周围的上下文词汇分布会有显著差异。
*   **案例 "Bank"（银行 vs 河岸）**：
    *   与 *money, loan, mortgage, rate* 共现频率高 -> 聚为一类（银行）。
    *   与 *water, river, boat* 共现频率高 -> 聚为一类（河岸）。
    *   *注*：像 *building* 这种词区分度不高，因为银行和河岸都可能有建筑物。
*   **方法**：
    *   Agglomerative Clustering（凝聚聚类）。
    *   利用相似度计算，自动将上下文聚成不同的簇（Clusters）。
*   **优缺点**：
    *   *优点*：完全无需人工干预。
    *   *缺点*：
        *   机器分出的“类”不一定对应词典中的准确“义项”。
        *   可能分出无意义的类，或将两个义项混为一类。
*   **对比**：
    *   最常见义项法（Baseline）：正确率约 60%-70%。
    *   无指导学习（Schütze等）：正确率可达 90% 以上（针对某些词）。
    *   **结论**：即使是“懒人”方法（无指导），依靠数据统计的力量也比单纯的主观猜测（最常见义项）要强。

---

## 四、 拓展讨论：计算语言学的学科反思

课程最后引用了一篇重要文章，探讨计算语言学的本质。

*   **文章**：Shuly Wintner (2009). *"What Science Underlies Natural Language Engineering?"* (自然语言工程的支撑科学是什么？)
*   **背景**：
    *   在 EACL 2009 会议上，有人提议在 ACL（国际计算语言学协会）下成立一个“语言学特别兴趣小组”（SIG on Linguistics）。
    *   **讽刺的比喻**：这就像在美国儿科学会下成立一个“医学特别兴趣小组”。儿科本应基于医学，计算语言学本应基于语言学。
*   **现状**：
    *   计算语言学（自然语言处理）领域中，统计方法和工程方法占据主导，语言学似乎被遗忘了。
    *   “儿子（计算语言学）长大了，觉得妈妈（语言学）没用，搞统计去了。”
*   **呼吁**：
    *   **"Call for the return of linguistics to computational linguistics"**。
    *   呼吁让语言学回归计算语言学的怀抱。
*   **老师的观点**：
    *   计算语言学的对象是带有文化色彩的“语言”，不是扑克牌或基因序列。
    *   不能因为统计方法有效就完全抛弃语言学理论。
    *   我们需要语言学作为知识支撑，即使是计算机专家（如Wintner）也意识到了这一点。

---

**总结**：
本节课通过词义消歧（WSD）这一具体任务，展示了计算语言学方法论的演进：从人工规则 -> 利用词典 -> 有指导机器学习 -> 半指导/无指导学习。最后上升到哲学高度，提醒研究者在追求工程效果（统计/机器学习）的同时，不可数典忘祖，忽略了语言学本身的理论指导作用。


以下是根据课堂录音文稿整理的计算语言学课程知识要点笔记（第三十一、三十二部分）：

# 计算语言学课程笔记（续）

## 一、 词义消歧与评测 (Word Sense Disambiguation & Evaluation)

### 1. 基于词典的消歧方法
*   **方法**：利用定义完善、释义规范的机器可读词典（如《现代汉语词典》、《规范词典》）进行词义消歧。
*   **原理**：通过对比上下文与词典中的义项定义来确定词义。例如判断 "win" 是修饰动词还是名词，查阅词典即可解决很多规则难以处理的问题。
*   **建议**：适合作为博士或毕业论文题目。虽然编程技巧要求不高，但考察语言分析能力和查询匹配技巧。

### 2. 国际评测：Senseval / SemEval
*   **背景**：词义消歧（WSD）和词语排序是自然语言处理中的难点，对机器翻译（MT）和信息检索（IR）至关重要。
*   **组织**：ACL 下属的 **SIGLEX** (Special Interest Group on the Lexicon)。
*   **发展历程**：
    *   **Senseval**：始于1998年，进行了三届。
    *   **SemEval (Semantic Evaluation)**：2007年第四届起改名。
*   **任务范围**：不仅包含词义消歧，还扩展到语义角色标注、情感分析、时间关系分析、Web检索等。
*   **现状**：
    *   虽然每年有评测，但词汇语义问题仍未完全解决，缺乏像句法分析中CYK算法那样通用的、有力的统一方法。
    *   中国学者（如台湾的黄居仁老师）在其中发挥了重要作用，大陆学者参与度也在提升。

## 二、 形态自动处理 (Automatic Morphological Analysis)

本章是课程的重头戏，主要包含以下内容：
1.  **分词与形态还原 (Tokenization & Lemmatization)**：
    *   汉语需要分词（Word Segmentation）。
    *   英语等语言需要处理词形变化（如 -s, -ed, -ing），将词还原为原形（词干）。
2.  **词类标注 (Part-of-Speech Tagging)**：
    *   解决“兼类词”问题（如“科学”既可是名词也可是形容词；“白”既可是形容词也可是副词）。
    *   **核心理论**：有限状态自动机（FSA）与隐马尔可夫模型（HMM）。

## 三、 有限状态自动机 (FSA) 的理论发展史

形态分析最重要的理论基础是**有限状态自动机**，其思想发展经历了漫长的过程：

1.  **图灵 (Alan Turing, 1936)**：提出**图灵机 (Turing Machine)**。
    *   模型：控制器 + 无限长的纸带（Tape）+ 读写头。
    *   意义：建立了计算的基本模型。
2.  **冯·诺依曼 (Von Neumann)**：解决程序存储问题，提出程序本身也可以作为数据存储，奠定了现代计算机基础。
3.  **麦卡洛克与皮茨 (McCulloch & Pitts, 1943)**：提出神经元网络模型（MP模型），探讨信息的表达与传递。
4.  **克林 (Kleene, 1951)**：
    *   定义了**有限状态自动机**和**正则表达式 (Regular Expression)**。
    *   证明了二者的等价性。
    *   影响：为后来的编译器（如 grep, UNIX工具）奠定了基础。
5.  **香农 (Shannon) & Huffman**：开关电路与机电系统的代数模型。
6.  **摩尔 (Moore, 1956) & Mealy**：提出有限状态自动机的具体形式（Moore机和Mealy机）。

## 四、 形态分析与标注系统的演进

### 1. 早期系统与规则方法
*   **Affix Stripping (词缀剥离法)**：最早的方法，通过切除前缀后缀并查找词根表来分析。
*   **Kimmo System (Two-level Morphology)**：
    *   基于 Kaplan & Kay 以及 Koskenniemi (1983) 的工作。
    *   使用有限状态转换器（Finite State Transducer），处理音系规则和拼写规则，能进行双向分析（生成与识别）。
*   **TDAP (Transformation and Discourse Analysis Project)**：Harris (乔姆斯基的老师) 在宾夕法尼亚大学开发（1958-1959），实现了最早的英语形态分析和词类消歧规则。
*   **CGC (Computational Grammar Coder)**：Klein & Simmons (1962)，结合词典和上下文规则。
*   **TAGGIT (1971)**：
    *   用于标注 **Brown Corpus**。
    *   基于规则（例如：`VBZ` 后不能接 `NNS`）。
    *   正确率：约 **77%**。

### 2. 统计方法的兴起
*   **CLAWS (Constituent Likelihood Automatic Word-tagging System)**：
    *   用于标注 **LOB Corpus** (Lancaster-Oslo/Bergen Corpus)。
    *   采用了概率统计方法（HMM思想）。
    *   正确率：提高到 **90% 以上**。
*   **PARTS**：Church (1988)，改进的HMM算法。

### 3. 混合与学习方法
*   **TBL (Transformation-Based Learning, 1997)**：Brill 提出。
    *   **思想（绘画隐喻）**：
        1.  先用最简单的算法给所有词标一个默认标记（如全标为名词），好比先把画布全涂蓝。
        2.  发现错误（如房子应该是白色），用规则进行修正。
        3.  通过机器学习自动学习修正规则。
    *   优点：结合了规则与统计的优势，逻辑清晰，效果好。

### 4. 现代模型
*   **HMM (隐马尔可夫模型)**
*   **Maximum Entropy (最大熵模型)**
*   **MEMM (最大熵马尔可夫模型)**

## 五、 语言类型学与形态处理 (Linguistic Typology)

不同类型的语言决定了形态处理的复杂度：

1.  **分析语 (Analytic/Isolating Languages)**：如**汉语**。
    *   特点：缺乏词形变化，依靠语序和虚词（如“着、了、过”）表达语法关系。
    *   处理重点：分词与虚词分析。
2.  **黏着语 (Agglutinating Languages)**：如**土耳其语、日语、韩语**。
    *   特点：词干后粘贴多个词缀，每个词缀表示单一语法意义（“一意一形”）。
    *   **案例（土耳其语）**：一个词（如 `civilized...`）可以包含“变成”、“使役”、“否定”、“过去分词”、“人称”、“数”等一长串后缀。
    *   **处理挑战**：无法将所有变化形式存入词典（会产生组合爆炸）。必须使用**形态解析 (Morphological Parsing)**，切分词尾并识别其语法意义。
3.  **屈折语 (Inflectional/Fusional Languages)**：如**英语**（现代英语向分析语过渡）、拉丁语。
    *   特点：一个词缀可以表示多个语法意义（如英语 `-s` 可表示复数、第三人称单数、所有格）。
    *   处理重点：消除词缀的多义性。

## 六、 有限状态自动机的应用实例
*   **英语形态分析模型**：
    *   可以用一个简单的四状态自动机来描述英语构词法：
        *   State 0 -> State 1 (Prefix -> Root)
        *   State 1 -> State 2 (Root -> Suffix)
        *   State 2 -> Final (Suffix -> End)
    *   **例子**：*Re-form-ation-s*
        *   *re-* (前缀)
        *   *form* (词根)
        *   *-ation* (名词后缀)
        *   *-s* (复数后缀)
    *   通过FSA控制状态转移，可以有效识别和生成合法词形。

以下是根据课堂录音文稿（33-34）整理的计算语言学课程知识要点笔记：

# 计算语言学课程笔记：形态分析与正则表达式

## 一、 形态分析 (Morphological Analysis)

### 1. 复合词的处理 (Compound Words)
*   **处理机制**：利用有限状态自动机（FSA）。
*   **流程**：
    *   分析第一个词素（如 *glass*），从状态0走到中间状态。
    *   分析完第一个词素后，路径回到状态0，再开始分析第二个词素（如 *light*）。
    *   **案例**：
        *   *glasslight*：glass (0 -> F) + light (0 -> F) -> 结束。
        *   *hardwood*：hard (0 -> F) + wood (0 -> F)。

### 2. 形态分析的四个主要步骤
1.  **分词 (Tokenization)**：
    *   任务：确定词的边界（Word Segmentation）。
    *   **挑战**：
        *   中文：没有空格，前后缀识别困难。
        *   英文：虽然有空格，但也存在复杂情况。
            *   *数字与符号*：`123,456.78`（算一个词还是多个？），百分号 `%`。
            *   *日期*：`3/8`（3月8日还是8月3日？）。
            *   *缩写与省略*：`let's` -> `let us`，`I'm` -> `I am`。
2.  **词目还原 (Lemmatization)**：
    *   任务：将变形词还原为原形（Lemma），并提取语法特征。
    *   **案例**：
        *   *tablets* $\rightarrow$ `table` + N (名词) + PL (复数)。
        *   *strongest* $\rightarrow$ `strong` + ADJ (形容词) + Superlative (最高级)。
        *   *cats* $\rightarrow$ `cat` + N + PL。
        *   *merging* $\rightarrow$ `merge` + V + Present Participle (现在分词)。
    *   *歧义处理*：如 *leaves* (可能还原为 `leaf`+N+PL 或 `leave`+V+3P+SG)，需列出所有可能性。
3.  **词性标注 (POS Tagging)**：给词语标上词性（Part-of-Speech）。
4.  **词性消歧 (POS Disambiguation)**：确定在特定上下文中的唯一词性。

## 二、 正则表达式 (Regular Expressions)

### 1. 定义与重要性
*   **地位**：计算语言学的基础工具，任何学计算机或NLP的学生必须掌握。
*   **本质**：正则表达式与**有限状态自动机 (Finite State Automata, FSA)** 是等价的。
*   **应用**：UNIX系统、Microsoft Word、文本搜索等。

### 2. 基本语法与符号 (Basic Syntax)
*   **普通字符**：直接匹配（如 `/woodchuck/`）。
*   **方括号 `[]` (Disjunction)**：
    *   表示“或”的关系，匹配括号内任意一个字符。
    *   **区分大小写**：`[Ww]oodchuck` 可匹配 *Woodchuck* 或 *woodchuck*。
    *   **范围 (Range)**：利用连字符 `-`。
        *   `[A-Z]`：所有大写字母。
        *   `[a-z]`：所有小写字母。
        *   `[0-9]`：所有数字。
*   **脱字符 `^` (Negation)**：
    *   放在方括号**最前面**表示否定（不包含）。
    *   例：`[^A]` 表示任何非A的字符。
    *   *注意*：如果 `^` 不在方括号开头，则仅表示字符 `^` 本身。
*   **问号 `?` (Optionality)**：
    *   表示前面的字符可有可无（0次或1次）。
    *   例：`woodchucks?` 匹配 *woodchuck* 或 *woodchucks*；`colou?r` 匹配 *color* 或 *colour*。

### 3. 高级计数与匹配 (Kleene Operators & Wildcards)
用于描述重复出现的字符（如模拟羊叫声 "baaa!"）：
*   **Kleene Star `*` (克林星号)**：
    *   表示前面的字符出现 **0次或无限次**。
    *   例：`ba*` 匹配 *b*, *ba*, *baa*, *baaa*...
*   **Kleene Plus `+` (克林加号)**：
    *   表示前面的字符出现 **1次或无限次**（不包含0次）。
    *   例：`ba+` 必须至少有一个a。
*   **点号 `.` (Wildcard)**：
    *   通配符，表示除换行符外的任意单个字符。
    *   例：`beg.n` 可匹配 *begin*, *began*, *begun*。

### 4. 替换与记忆 (Substitution & Memory)
*   **替换语法**：`s/pattern/replacement/`。
*   **记忆寄存器 (Registers/Capture Groups)**：
    *   使用圆括号 `()` 将模式包裹，通过 `\1`, `\2` 等引用被匹配的内容。
    *   例：将文本中的数字两边加上括号。
    *   例：`the (.*) they were` $\rightarrow$ 提取中间的形容词。

## 三、 案例分析：ELIZA 系统 (1966)

*   **背景**：由 Weizenbaum 开发，模拟罗杰斯式心理医生（Rogerian Psychotherapist）。
*   **核心技术**：**正则表达式的替换 (Pattern Matching & Substitution)**。
*   **工作原理**：
    *   通过关键词匹配和预设的替换规则与用户对话，制造“理解”的假象。
*   **对话示例**：
    *   用户："I'm **depressed**." $\rightarrow$ 机器匹配规则 `s/.* I'm (.*)/I'm sorry to hear you are \1/` $\rightarrow$ 回复："I'm sorry to hear you are **depressed**."
    *   用户："All men are alike." $\rightarrow$ 机器匹配 `all` $\rightarrow$ 回复："In what way?" (诱导性提问)。
    *   用户："They are **always** bugging us." $\rightarrow$ 机器匹配 `always` $\rightarrow$ 回复："Can you think of a specific example?"
*   **结论**：机器并不理解语义，只是进行了形式上的符号替换，但能产生以假乱真的效果。

## 四、 有限状态自动机 (Finite State Automata, FSA)

### 1. 理论关系
*   **等价性**：正则表达式 $\Leftrightarrow$ 有限状态自动机 (FSA) $\Leftrightarrow$ 正则语言 (Regular Language / Type 3 Grammar)。
*   乔姆斯基分层中的第三型文法（正则文法）对应的识别器就是有限状态自动机。

### 2. 形式化描述
可以用一个五元组来定义 FSA：
*   **Q**：有限的状态集合 ($q_0, q_1...q_n$)。
*   **$\Sigma$ (Sigma)**：有限的输入符号字母表 (Input Alphabet)。
*   **$q_0$**：初始状态 (Start State)。
*   **F**：终结状态集合 (Final States)。
*   **$\delta$ (Delta)**：转移函数 (Transition Function)，描述在某个状态下输入某个符号后跳转到哪个状态。

### 3. 羊叫声识别案例 (Sheep Language)
*   目标语言：`baaa...!` (b + 至少两个a + !)
*   **正则表达式**：`baa+!` 或 `baaa*!`
*   **FSA 状态转移**：
    *   State 0: 输入 'b' $\rightarrow$ State 1
    *   State 1: 输入 'a' $\rightarrow$ State 2
    *   State 2: 输入 'a' $\rightarrow$ State 3
    *   State 3: 输入 'a' $\rightarrow$ State 3 (循环/自环，处理无限个a)
    *   State 3: 输入 '!' $\rightarrow$ State 4 (终结状态)
    *   其他情况进入死状态或拒绝识别。

这是根据提供的课堂录音文稿（第35、36部分）整理的计算语言学课程知识要点笔记。

# 计算语言学课程笔记：有限状态自动机与形态分析

## 一、 有限状态自动机 (FSA) 的深化讨论

### 1. 状态转移与“羊语言”案例
*   **案例回顾**：描述一种模拟羊叫的语言（如 `baaa!`）。
*   **状态流程**：
    *   $Q_0 \xrightarrow{b} Q_1$
    *   $Q_1 \xrightarrow{a} Q_2$
    *   $Q_2 \xrightarrow{a} Q_2$ (循环读取 'a')
    *   $Q_2 \xrightarrow{a} Q_3$
    *   $Q_3 \xrightarrow{!} Q_4$ (结束)
*   **错误状态处理 (Failure State)**：
    *   引入 $Q_F$ (Failure State) 表示失败/死胡同。
    *   如果输入字符在当前状态下没有定义的转移路径，则进入 $Q_F$。
    *   例如：在 $Q_1$ 读到 '!' 或在 $Q_4$ 继续读入字符，均导致失败。

### 2. 形式语言的定义
*   **定义**：形式语言是符号串的集合，每个符号串由字母表（Alphabet）中的有限符号组成。
*   **记法**：$L(M)$ 表示由模型 $M$（如有限状态自动机）所刻画/生成的语言。
*   **乔姆斯基体系 (Chomsky Hierarchy)**：
    *   3型语言：正则语言 (Regular Language) / 有限状态语言。
    *   2型语言：上下文无关语言 (Context-Free)。
    *   1型语言：上下文有关语言 (Context-Sensitive)。
    *   0型语言：递归可枚举语言 (Recursively Enumerable)。

## 二、 FSA 应用案例：英语数字识别系统
老师演示了如何构建一个识别 1 到 99 英语数字的有限状态机。

### 1. 基础数字 (1-19)
*   **1-9**：直接从初始状态识别 (one, two, ... nine)。
*   **11-19**：
    *   11, 12 (eleven, twelve) 单独处理。
    *   13-19 (thirteen... nineteen) 识别后缀 `-teen`。

### 2. 两位数 (20-99)
*   **整十数**：20-90 (twenty, thirty... ninety) 识别后缀 `-ty`。
*   **组合数**：
    *   逻辑路径：先识别整十数 (如 twenty)，进入中间状态，再通过连字符 (hyphen) 或直接连接 1-9 (one... nine)。
    *   **路径约束**：例如 `fifteen` 后面不能再跟 `one`，必须在 `fifty` 后面跟。

### 3. 拓展应用 (Extension)
*   **货币表示**：
    *   增加 `cent` (美分)、`dollar` (美元) 的识别路径。
    *   可以组合：Dollars + Cents。
*   **大数字**：
    *   增加 `hundred`, `thousand` 等单位，逻辑同理，但状态图会变得更复杂。

## 三、 非确定性有限状态自动机 (NFSA / NFA)

### 1. 定义与特征
*   **非确定性 (Non-deterministic)**：在同一状态下，读入相同的符号，可以转移到不同的状态。
*   **现象**：机器在某一点面临选择，出现“犹豫不决”。
*   **成因**：
    1.  **同输入多路径**：例如在 $Q_2$ 读入 'a'，既可以留在 $Q_2$，也可以进入 $Q_3$。
    2.  **空转移 ($\epsilon$-transition)**：不读入任何字符就自动跳转状态（Jump）。可能导致机器在不读入符号的情况下在状态间循环或回退。

### 2. 解决非确定性的搜索策略
将识别过程视为在**状态空间 (State Space)** 中的搜索问题。

*   **深度优先搜索 (Depth First Search, DFS)** / **Backup (回溯)**：
    *   策略：选定一条路一直往下走，直到成功或失败。
    *   如果失败（走进死胡同），则**回退 (Backtrack)** 到上一个选择点（Decision Point），尝试另一条路径。
    *   **要求**：需要记录每一个选择点（状态节点 + 输入带子的位置），以便恢复现场。
*   **Look Ahead (前瞻)**：
    *   在做决定前，多往后读几个字符，看哪条路通。
*   **广度优先搜索 (Breadth First Search, BFS) / Parallelism (并行)**：
    *   策略：同时走所有可能的路径。
    *   如同“分身术”，保留所有可能的结果集合，看哪一个集合能最终走到终点。

### 3. NFA 与 DFA 的关系
*   **等价性**：任何非确定性自动机 (NFA) 都可以转化为等价的确定性自动机 (DFA)。
*   **转化方法**：**状态合并**。将 NFA 中某种输入导致的所有可能状态集合合并为一个新的单一状态（如将 $Q_2$ 和 $Q_3$ 合并为 $Q_{23}$）。

## 四、 正则语言 (Regular Language) 的数学性质
正则语言与有限状态自动机存在严格的对应关系。

### 1. 封闭性 (Closure Properties)
若 $L_1$ 和 $L_2$ 是正则语言，则以下运算结果仍然是正则语言（老师列举了数学结论，未做证明）：
*   **连接 (Concatenation)**：$L_1L_2$。
*   **析取/并集 (Union/Disjunction)**：$L_1 \cup L_2$。
*   **克林闭包 (Kleene Star)**：$L_1^*$ (即肯定闭包，包括空串)。
*   **交集 (Intersection)**：$L_1 \cap L_2$。
*   **差集 (Difference)**：$L_1 - L_2$。
*   **补集 (Complementation)**：$\Sigma^* - L_1$。
*   **逆/倒置 (Reversal)**：$L_1^R$ (将所有字符串倒序)。

### 2. 自动机的构建运算
通过连接两个 FSA 来构建新的 FSA：
*   **毗连 (Concatenation)**：将 FSA1 的终态通过空转移 ($\epsilon$) 连接到 FSA2 的初态。
*   **闭包 (Closure)**：将终态通过空转移连回初态，形成循环。
*   **结合/并 (Union)**：新建一个初态 $Q_0$，通过空转移分别连接到 FSA1 和 FSA2 的初态。

## 五、 英语形态分析 (Morphological Analysis)

### 1. 语素分类
英语语素分为 **词干 (Stem)** 和 **词缀 (Affix)**。

### 2. 词缀的类型
*   **前缀 (Prefix)**：
    *   加在词头。例：`un-` (unblock)。
*   **后缀 (Suffix)**：
    *   加在词尾。例：`-s` (cats), `-ed`。
    *   *注*：形态分析中通常将屈折变化（如复数、时态）处理为后缀。
*   **中缀 (Infix)**：
    *   加在词中间。
    *   英语中较少见，口语案例：`abso-bloody-lutely` (虽然老师举例 `Halle-bloody-lujah` 类比，指出这是社会语言学现象)。
    *   Tagalog (他加禄语) 中常见，如 `um` 插在中间。
*   **周缀/围缀 (Circumfix)**：
    *   老师命名为“围缀”。加在词干的前后。
    *   案例：**德语 (German)** 的过去分词。
        *   *sagen* (说) -> *gesagt* (说过)。
        *   词干是 *sag*，前后同时加上 *ge-* 和 *-t*。

### 3. 拼接性 (Concatenation)
*   语素通常是线性拼接的（Concatenative），这也是有限状态自动机能处理它们的基础。

以下是根据课堂录音文稿（File 37 & 38）整理的计算语言学课程知识要点笔记。

# 计算语言学课程笔记（三十七、三十八）

## 一、 有限状态自动机（FSA）的进阶理论与搜索

### 1. 非确定性有限状态自动机 (NFA/NFSA)
*   **概念**：在同一状态下，读入同一个符号，可能进入两个或多个不同的状态（非决定性）。
*   **问题**：识别过程变为一个搜索问题（Search Problem），需要在状态空间（State Space Search）中寻找一条通往终态的路径。
*   **搜索策略**：
    *   **深度优先搜索 (Depth First Search, DFS)**：一条路走到黑，撞墙（失败）后回溯（Backtracking），尝试另一条路径。
    *   **广度优先搜索 (Breadth First Search, BFS)** / 平行搜索：同时尝试所有可能的路径（如同时保留两个可能的状态结果），看哪条能走到最后。
*   **确定化 (Determinization)**：
    *   将非确定性自动机转化为确定性自动机（DFA）。
    *   **方法**：状态合并（Subset Construction）。例如，若状态Q2读入'a'可去Q2或Q3，则将{Q2, Q3}合并为一个新状态。

### 2. 正则语言与FSA的数学性质
*   **等价性**：正则表达式（Regular Expression） $\Leftrightarrow$ 有限状态自动机（FSA） $\Leftrightarrow$ 正则语言（Regular Language）。
*   **封闭性运算 (Closure Properties)**：
    *   如果L1和L2是正则语言，经过以下运算后的结果仍然是正则语言：
        *   **并集 (Union)**：$L1 \cup L2$
        *   **交集 (Intersection)**：$L1 \cap L2$
        *   **差集 (Difference)**：$L1 - L2$
        *   **补集 (Complementation)**：$\Sigma^* - L1$
        *   **连接 (Concatenation)**：$L1L2$
        *   **Kleene闭包 (Kleene Closure)**：$L^*$ (包含空串的重复)
        *   **逆 (Reversal)**：$L^R$
*   **自动机运算实现**：
    *   **连接**：将FSA1的终态通过$\epsilon$（空跳）连接到FSA2的初态。
    *   **闭包**：将终态通过$\epsilon$跳回初态。
    *   **并集**：新建一个起始状态，分叉连接到两个自动机的初态。

---

