## 二、 英语形态学 (English Morphology) 基础

### 1. 语素与词缀 (Morphemes & Affixes)
*   **词干 (Stem)**：词的核心部分，承载基本词义。
*   **词缀 (Affix)**：
    *   **前缀 (Prefix)**：如 *un-*believable。
    *   **后缀 (Suffix)**：如 eat-*ing*。
    *   **中缀 (Infix)**：英语中很少见，通常用于俚语或强调（如 *abso-bloody-lutely*）。
    *   **围缀 (Circumfix)**：前后同时加缀。英语极少，德语常见（如 *ge-sag-t*）。

### 2. 形态学的分类
*   **拼接性形态学 (Concatenative Morphology)**：
    *   如英语，主要通过前后缀拼接形成新词。
*   **非拼接性形态学 (Non-concatenative Morphology)**：
    *   **典型案例：阿拉伯语 (Arabic)**。
    *   **Root-and-Pattern (词根-模式) 系统**：
        *   **词根**：通常由3个辅音（Consonants）组成，如 *L-M-D* (表示“学”或“教”相关的基本义)。
        *   **模式**：在辅音中间插入元音（Vowels）来表达语法意义（如时态、语态）。
        *   *例子*：插入不同元音可变为“他学习了”、“他正在学”、“被教”等。这种机制对处理拼接语言的系统（及欧美人/中国人）来说较难掌握。

---

## 三、 曲折变化 (Inflection) 与 派生 (Derivation)

### 1. 曲折变化 (Inflection)
*   **定义**：不改变词类，只改变语法意义（如数、格、时态）。
*   **英语名词 (Nouns)**：
    *   **复数**：
        *   规则：加 -s 或 -es (结尾为s, z, x, ch, sh等)。
        *   不规则：*mouse/mice*, *ox/oxen*, *goose/geese*, *sheep/sheep* (单复同形)。
    *   **所有格**：加 *'s*。
*   **英语动词 (Verbs)**：
    *   **分类**：主要动词 (Main verbs)、情态动词 (Modal verbs: can, will)、助动词 (Primary verbs: be, have, do)。
    *   **形式**：
        *   **规则动词** (4种形式)：Stem (*walk*), -s (*walks*), -ing (*walking*), -ed (*walked* - 过去式/过去分词同形)。
        *   **不规则动词** (5-8种形式)：
            *   *cut* (3种)：cut, cuts, cutting (过去式/分词不变)。
            *   *eat* (5种)：eat, eats, eating, ate, eaten。
            *   *be* (8种)：am, is, are, was, were, be, being, been。
*   **英语形容词 (Adjectives)**：
    *   比较级 (-er)、最高级 (-est)、副词化 (-ly)。
    *   不规则：*good/better/best*。

### 2. 派生 (Derivation)
*   **定义**：通常会改变词类，或显著改变词汇意义。
*   **名词派生**：
    *   动词 -> 名词：*computerize* -> *computerization* (-ation)。
    *   形容词 -> 名词：*fuzzy* -> *fuzziness* (-ness)。
*   **形容词派生**：
    *   名词 -> 形容词：*computation* -> *computational* (-al)。
    *   动词 -> 形容词：*embrace* -> *embraceable* (-able)。
*   **特点**：比起曲折变化，派生更复杂，更难用简单的规则完全覆盖。

### 3. 附着词/接续词 (Cliticization)
*   *注：录音中发音似 "click decision"，实指 Cliticization。*
*   **定义**：在语音上依赖于相邻词的短语成分。
*   **分类**：
    *   **前附着 (Proclitics)**。
    *   **后附着 (Enclitics)**：如 *'s*, *'re*, *n't*。
*   **处理**：需要将其还原为完整形式以便词典查询。
    *   *Examples*：
        *   *he's* -> *he is* 或 *he has*。
        *   *can't* -> *can not*。
        *   *d'you* -> *do you*。
        *   *I'm* -> *I am*。

---

## 四、 计算形态分析的实现 (Computational Morphology)

### 1. 目标
*   **输入**：表面形式 (Surface Form)，如 *cats*, *walking*, *biggest*。
*   **输出**：词汇形式 (Lexical Form)，包含原形和形态特征，如 *cat +N +PL*, *walk +V +Prog*, *big +Adj +Sup*。

### 2. 所需的三个组件
要构建一个形态分析器 (Morphological Parser)，需要：
1.  **词表 (Lexicon)**：包含词干（stems）和词缀（affixes）的基本信息。
2.  **形态配列规则 (Morphotactics)**：规定语素结合的顺序。
    *   例如：复数后缀 *-s* 必须跟在名词词干后面，不能在前面。
    *   模型：利用有限状态自动机 (FSA) 建模，定义合法的路径。
3.  **正字法规则 (Orthographic Rules)**：处理拼写变化。
    *   例如：*city* + *s* -> *cities* (y变i加es)；*make* + *ing* -> *making* (去e)。

### 3. 有限状态转换机 (Finite State Transducer, FST)
*   **应用**：使用 FST 将“表面带” (Surface Tape) 映射到 “词汇带” (Lexical Tape)。
*   **建模示例**：
    *   **名词模型**：
        *   路径1（规则）：词干 -> 复数后缀 (-s/-es) -> 输出 +PL。
        *   路径2（不规则）：直接识别 *geese* -> 输出 *goose +N +PL*。
    *   **动词模型**：
        *   设计路径分别处理：原形、单三形式 (-s)、进行时 (-ing)、过去式/分词 (-ed)。
        *   不规则动词需单独路径（如 *cut* 的过去式仍是 *cut*）。
    *   **形容词模型**：
        *   前缀 (un-) -> 词干 -> 后缀 (-er, -est, -ly)。

## 五、 后续课程安排
*   **词性标注 (POS Tagging)**：基于规则的方法 vs 基于统计的方法。
*   **句法分析 (Syntactic Parsing)**：如果时间允许。


以下是根据课堂录音文稿（39-40）整理的计算语言学课程知识要点笔记：

# 计算语言学课程笔记：形态分析与有限状态转录机

## 一、 英语形态变化的有限状态自动机 (FSA) 描述

老师首先回顾了形态分析，并展示了如何用**有限状态自动机 (Finite State Automata, FSA)** 来描述英语中的屈折变化（Inflection）和派生变化（Derivation）。

### 1. 英语屈折变化的 FSA 描述
针对不同词性的变化规则，老师展示了相应的自动机模型：

*   **名词 (Nouns)**
    *   **规则变化 (Regular)**：
        *   单数：原形（如 *fox, cat, dog*）。
        *   复数：加 `s` 或 `es`（如 *foxes, cats, dogs*）。
    *   **不规则变化 (Irregular)**：
        *   元音变化：*goose* $\rightarrow$ *geese*, *mouse* $\rightarrow$ *mice*。
        *   单复数同形：*sheep* $\rightarrow$ *sheep*。
*   **动词 (Verbs)**
    *   **规则变化**：
        *   过去式 (Past) / 过去分词 (Past Participle)：加 `ed`（如 *worked, created*）。
        *   现在分词 (Progressive)：加 `ing`（如 *working*）。
        *   第三人称单数 (3rd Person Singular)：加 `s`（如 *works*）。
    *   **不规则变化**：
        *   完全变形：*catch* $\rightarrow$ *caught*, *eat* $\rightarrow$ *ate/eaten*。
        *   不变形：*cut* $\rightarrow$ *cut*。
        *   *注*：部分不规则动词的现在分词和第三人称单数仍遵循规则（如 *eating, eats*）。
*   **形容词 (Adjectives)**
    *   **基本变化**：比较级加 `er`，最高级加 `est`，副词加 `ly`。
    *   **前缀处理**：如 `un-` (unreal, unbig)。
    *   **复杂性处理**：
        *   并非所有形容词都能加 `un-`（如 *big* 不能变成 *unbig*）。
        *   并非所有形容词都能加 `ly`（如 *rare* $\rightarrow$ *rarely* 可行，但某些不可以）。
        *   **解决方案**：在 FSA 中将形容词词干分为不同类别（如 `Adj_root_1` 允许加前缀/后缀，`Adj_root_2` 不允许），通过不同路径处理。

### 2. 英语派生变化的 FSA 描述
派生变化涉及词性转换（如名词变动词），比屈折变化更复杂。老师展示了一个包含11个状态（Q0-Q11）的复杂自动机来描述这一过程。

*   **派生路径示例**：
    *   **N $\rightarrow$ V $\rightarrow$ N**: *fossil* (N) + `-ize` $\rightarrow$ *fossilize* (V) + `-ation` $\rightarrow$ *fossilization* (N)。
    *   **Adj $\rightarrow$ N**: *equal* (Adj) + `-ity` $\rightarrow$ *equality* (N)。
    *   **Adj $\rightarrow$ N**: *natural* (Adj) + `-ist` $\rightarrow$ *naturalist* (N)。
    *   **V $\rightarrow$ Adj $\rightarrow$ N**: *realize* (V) + `-able` $\rightarrow$ *realizable* (Adj) $\rightarrow$ *realizability* (N)。
*   **方法论优势**：相比传统语法学家用文字描述规则，用 FSA 图描述更清晰、可编程、可验证（穷尽路径）。
*   **实践中的挑战**：
    *   实际构建系统时，需要处理**正词法 (Orthography)** 问题（如 *cry* + *ing* 不变，但 *cry* + *ed* 变成 *cried*；*frame* + *ing* 去掉 e 变成 *framing*）。
    *   需要建立“子词表 (Sub-word list)”来具体识别语素。

---

## 二、 从 FSA 到有限状态转录机 (FST)

FSA 只能识别（Accept/Reject）一个字符串是否符合规则，但在形态分析中，我们需要**输出**分析结果（如输入 *cats*，输出 *cat +N +PL*）。因此引入了**有限状态转录机 (Finite State Transducer, FST)** 和**双层形态学**理论。

### 1. 双层形态学 (Two-level Morphology)
由 Kimmo Koskenniemi (1983) 提出，核心思想是建立两个层面的对应关系：
*   **词汇层 (Lexical Level)**：底层的语法/形态表示。
    *   例：`cat +N +PL`
*   **表层 (Surface Level)**：实际书写/发音形式。
    *   例：`cats`
*   **对应关系**：
    *   **生成 (Generation)**：从词汇层 $\rightarrow$ 表层。
    *   **分析 (Analysis)**：从表层 $\rightarrow$ 词汇层。

### 2. 有限状态转录机 (FST) 的定义与特性
FST 是 FSA 的扩展，区别在于 FST 有**两条带子**（输入带和输出带），而 FSA 只有一条。

*   **数学定义**：
    *   有限状态集合 $Q$。
    *   字母表 $\Sigma$：由输入/输出对 $i:o$ 组成（Input:Output）。
    *   初始状态、终止状态。
    *   转换函数 $\delta$：在状态 $Q$ 遇到输入 $i$ 时，转入新状态 $Q'$ 并输出 $o$。
*   **FST 的四种功能**：
    1.  **识别器 (Recognizer)**：检查输入是否合法。
    2.  **生成器 (Generator)**：输入语法标记，输出单词（Lexical $\rightarrow$ Surface）。
    3.  **翻译器 (Translator)**：将一种符号串转换为另一种（如语言翻译）。
    4.  **关联器 (Relater)**：计算两个集合之间的关系。
*   **关键运算性质**：
    *   **逆运算 (Inversion)**：可以双向工作（生成与分析互逆）。
    *   **组合 (Composition)**：可以将多个转录机串联（如 output of T1 becomes input of T2）。

---

## 三、 FST 在形态分析中的具体应用

老师详细讲解了如何用 FST 描述英语名词的单复数变化，即构建一个 **FST for Number Processing**。

### 1. 规则名词的处理 (如 *fox, cat, dog*)
*   **词干映射**：
    *   输入（词汇层）：`f o x`
    *   输出（表层）：`f o x`
    *   对应关系：`f:f`, `o:o`, `x:x` （Identity mapping）。
*   **词尾映射**：
    *   若词汇层是 `+N +PL` (复数)：
        *   `+N` $\rightarrow$ $\epsilon$ (空)
        *   `+PL` $\rightarrow$ `s` (或根据正词法变为 `es`)
        *   最终表层输出：`foxes` (中间涉及边界符号 `^` 或 `#`)。

### 2. 不规则名词的处理 (如 *goose, mouse*)
利用 FST 的映射能力处理不规则变形：

*   **Goose (单数)**：
    *   映射：`g:g o:o o:o s:s e:e`
    *   后缀：`+SG` $\rightarrow$ `#` (结束)。
    *   结果：*goose*。
*   **Goose (复数 - Geese)**：
    *   映射：`g:g` `o:e` `o:e` `s:s` `e:e`
    *   后缀：`+PL` $\rightarrow$ `#`。
    *   结果：*geese*。
*   **Mouse (复数 - Mice)**：
    *   映射：`m:m` `o:i` `u:$\epsilon$` (u变为空) `s:c` `e:e`
    *   后缀：`+PL` $\rightarrow$ `#`。
    *   结果：*mice*。

### 3. FST 的优势
*   能够统一处理规则变化（附加词缀）和不规则变化（元音替换、词形改变）。
*   通过**组合 (Composition)** 操作，可以将“词干转录机”与“正词法规则转录机”（处理拼写规则如 `y->ie`, `+es`）串联起来，形成完整的形态分析系统。

---

## 四、 课堂讨论与观点
1.  **关于复杂语言的处理**：
    *   面对德语、俄语等具有复杂变格（如俄语名词单复数各6个格，共12个词尾）的语言，虽然FST图会很复杂，但原理相同。
    *   老师鼓励：只要有耐心，任何头脑清晰的人花1-2周都能画出这些语言的形态自动机。不要因为复杂就畏惧。
2.  **理论与实践的结合**：
    *   课堂上讲的是简化的原理图。
    *   实际工程（如机器翻译系统）中，面对数万词汇，需要极大的耐心建立详尽的词典和规则库，这是一个“天天熬”的过程。
3.  **FST 的必要性**：
    *   FSA 只能判断“是/不是”英语单词。
    *   FST 才能告诉我们“这个词的词干是什么，语法特征是什么”，这才是形态分析的核心目标。

以下是根据课堂录音文稿（41-42）整理的后续知识要点笔记：

## 六、 形态分析与有限状态机（进阶）

### 1. 级联有限状态转录机 (Cascaded FST)
*   **原理**：形态分析可以看作多个转录机（Transducer）的叠加或串联（Composition）。
*   **分层处理**：
    *   **词汇层 (Lexical Level)**：处理词干和词素组合（如 `fox + N + PL`）。
    *   **中间层 (Intermediate Level)**：连接词干与表层规则（如 `fox ^ s #`）。
    *   **表层 (Surface Level)**：最终的拼写形式（如 `foxes`）。
*   **数学逻辑**：利用转录机的组合性质，将词干处理（Stemming）和词尾处理（Suffix/Inflection）分开构建，再通过“级联”连接起来。

### 2. 正字法规则 (Orthographic Rules/Spelling Rules)
在实际工程中，不能简单地进行字符串拼接（如 `fox`+`s` 变成 `foxs` 是错误的），必须引入正字法规则处理拼写变化。老师列举了五种必须处理的英语拼写规则：
1.  **辅音重叠 (Consonant Doubling)**：如 `beg` -> `begging`。
2.  **E删除 (E-Deletion)**：在 `ing` 或 `ed` 前删除不发音的 `e`，如 `make` -> `making`。
3.  **E插入 (E-Insertion)**：在 `s`, `z`, `x`, `ch`, `sh` 后加复数或第三人称单数时需加 `e`，如 `fox` -> `foxes`, `watch` -> `watches`。
4.  **Y替换 (Y-Replacement)**：辅音+y 结尾，变 `y` 为 `ie` 或 `i`，如 `try` -> `tries` / `tried`。
5.  **K插入 (K-Insertion)**：较少见，如 `picnic` -> `picnicking`。

### 3. 三层形态学与规则描述
*   **方法**：引入一个**中间带 (Intermediate Tape)**。
    *   例如处理 `foxes`：
        1.  词汇层：`fox + N + PL`
        2.  中间层：`fox ^ s #` （`^`为语素边界，`#`为单词边界）
        3.  表层：`foxes` （在此过程中将边界符号 `^` 转化为 `e`）。
*   **规则写法**（Chomsky/Koskenniemi 风格）：
    *   `a -> b / c _ d`
    *   含义：当 `a` 出现在 `c` 之后、`d` 之前时，将其重写为 `b`。
    *   **E插入规则示例**：`ε -> e / {x, s, z} ^ _ s #`（在 x/s/z 后，且在语素边界和 s 之间，将空符号转换为 e）。

### 4. 形态分析中的歧义 (Ambiguity)
*   **问题**：同一个表层形式可能对应不同的词汇路径。
    *   **案例**：`foxes`。
        *   路径1（名词复数）：`fox + N + PL` （狐狸们）。
        *   路径2（动词第三人称单数）：`fox + V + 3SG` （使...困惑/欺骗）。例句："That trick foxes me every time."
*   **解决方案**：转录机必须能够涵盖所有可能的路径，具体的消歧依赖于词典（Dictionary）是否包含该词性的条目。

## 七、 计算语言学家的素养与定位
*   **学科交叉的必要性**：
    *   **纯计算机背景**：擅长算法，但往往忽略语言学细节（如拼写规则、不规则变化），认为太琐碎或无法处理。
    *   **纯语言学背景**：懂规则，但可能不懂形式化和实现。
*   **计算语言学家的角色**：做“计算语言学的语言学工作”，即用形式化的方法描述语言学规则（如编写FST规则）。
*   **案例**：老师派学生（传媒大学陈永钊）去比利时鲁文大学，专门负责编写语言学规则，因为当地计算机系学生无法胜任此工作。

## 八、 词类标注 (Part-of-Speech Tagging)

### 1. 问题定义
*   **词类消歧**：形态分析之后，进入句法分析之前，必须确定每个词的词性（Noun, Verb, etc.）。这是NLP的关键步骤。
*   **难点**：英语和汉语中都存在大量的**兼类词**（如 `fox` 既是名词也是动词）。

### 2. 词类标记集 (Tagset) 的现状

#### **汉语标记集**
*   **北京大学规范**：目前学术界使用最广，基于26个字母扩展，被许多语料库采用。
*   **国家/教育部规范**：
    *   **背景**：90年代中期由冯志伟老师负责协调制定。
    *   **原则**：**“协调第一，科学第二”**。
    *   **原因**：不同语言学家对词性分类（如名词是否该记为N）有不同见解，为了产业发展和国家利益，必须求同存异，制定统一的可交换标准（虽然目前国内仍存在多套标准并行，交换困难的问题）。

#### **英语标记集 (三大主流)**
老师重点介绍了英语中公认的三种标记集，并建议学生记忆其中一种：
1.  **Penn Treebank (宾州树库) Tagset**：
    *   **特点**：**45个标记**，最精简，使用最广泛。
    *   **建议**：**强烈建议博士生背诵或熟记这45个标记**，因为国际会议论文多用此标准，读懂它就不需要反复查表。
2.  **CLAWS C5 Tagset (Lancaster大学)**：
    *   **特点**：61个标记，中等规模。
    *   **应用**：用于 **BNC (British National Corpus)** 的概率标注。
3.  **CLAWS C7 Tagset**：
    *   **特点**：140多个标记，非常详细，历史上用于LOB语料库。

### 3. 学习建议
*   在国际发表论文或阅读文献时，了解行情（如Penn Treebank标记集）非常重要。
*   虽然汉语标准尚在协调中，但了解英语的成熟标准（特别是BNC和Penn Treebank）对研究有很大帮助。

以下是根据课堂录音文稿整理的计算语言学课程（第四十三、四十四部分）的知识要点笔记：

# 计算语言学课程笔记 (第四十三、四十四部分)

## 一、 词类标记集（Tagset）介绍
老师以**宾州树库（Penn Treebank）** 的词类标记集为例，讲解了词类标注的基本符号系统。

### 1. 常见标记符号
*   **连词类**：
    *   `CC`: Coordinating Conjunction（并列连词，如 and, but）。
    *   `IN`: Preposition/Subordinating Conjunction（介词或从属连词，如 in, or）。
*   **数词类**：
    *   `CD`: Cardinal Number（基数词，如 one, two）。
*   **限定词类**：
    *   `DT`: Determiner（限定词，如 the）。
    *   `PDT`: Predeterminer（前位限定词，如 **all** the... 中的 all）。
    *   `WDT`: Wh-determiner（如 which）。
*   **存在词**：
    *   `EX`: Existential there（存在句中的 there）。
*   **名词类**：
    *   `NN`: Noun, singular or mass（单数或不可数名词）。
    *   `NNS`: Noun, plural（复数名词）。
    *   `NNP`: Proper noun, singular（单数专有名词，如 IBM）。
    *   `NNPS`: Proper noun, plural（复数专有名词）。
*   **代词类**：
    *   `PRP`: Personal Pronoun（人称代词，如 I, he）。
    *   `PRP$`: Possessive Pronoun（所有格代词，如 my, his）。
    *   `WP`: Wh-pronoun（如 who）。
    *   `WP$`: Possessive wh-pronoun（如 whose）。
*   **形容词类**：
    *   `JJ`: Adjective（形容词）。
    *   `JJR`: Adjective, comparative（比较级）。
    *   `JJS`: Adjective, superlative（最高级）。
*   **副词类**：
    *   `RB`: Adverb（副词）。
    *   `RBR`: Adverb, comparative（比较级副词，如 faster）。
    *   `RBS`: Adverb, superlative（最高级副词，如 fastest）。
    *   `WRB`: Wh-adverb（疑问副词，如 how, where）。
*   **动词类**：
    *   `VB`: Verb, base form（动词原形）。
    *   `VBD`: Verb, past tense（过去式）。
    *   `VBG`: Verb, gerund/present participle（动名词/现在分词）。
    *   `VBN`: Verb, past participle（过去分词）。
    *   `VBP`: Verb, non-3rd person singular present（非第三人称单数现在时）。
    *   `VBZ`: Verb, 3rd person singular present（第三人称单数现在时）。
    *   `MD`: Modal（情态动词）。
*   **其他符号**：
    *   `TO`: to（作为不定式标记或介词）。
    *   `UH`: Interjection（感叹词）。
    *   `POS`: Possessive ending（所有格标记，如 's）。
    *   `RP`: Particle（小品词，如动词短语 give **up** 中的 up）。
    *   `SYM`: Symbol（符号）。
    *   标点符号：`$`, `#`, `(`, `)`, `,`, `.`, `` ` ``, `''` 等。

### 2. 学习建议
*   **记忆方法**：无需死记硬背，打印出来放在手边，使用两三天即可熟悉。
*   **标记长度**：通常由2-3个字母组成，有规律可循（如 N 代表名词，V 代表动词）。

## 二、 词类标注（Part-of-Speech Tagging）概述

### 1. 定义
*   **任务**：自动为文本中的每个单词分配正确的词类标记。
*   **本质**：这是一个**分类问题**（Classification Problem）。
*   **难点**：一个单词在词典中可能有多个词类（兼类词/歧义词），需要根据上下文消歧。
    *   例子：`books` 可能是名词（NNS），也可能是动词（VBZ）。
    *   例子：`that` 可能是限定词、关系代词、引导词或副词。

### 2. 词类标注的必要性
*   **工程角度**：
    *   为句法分析（Parsing）做准备。
    *   句法规则通常基于词类而非具体单词编写（否则规则数量会无限膨胀）。
    *   目的：减少后续处理的工作量，提高效率。
*   **学术争议**：
    *   **语料库驱动语言学（Corpus-driven Linguistics）**：部分学者认为词类是主观的，反对预先定义词类。
    *   **老师观点**：人类语言学两千年的成果（词类划分）大体正确且可继承，能够解决90%以上的问题，不必因噎废食。

### 3. 词类标注的难度与现状
*   **兼类统计**：
    *   **DeRose (1988)**：布朗语料库（Brown Corpus）中，按**词形（Type）**统计约11.5%有兼类歧义；按**词次（Token）**统计约40%有歧义。
    *   **结论**：大部分单词（约60%的Token）无歧义，但剩余40%需要处理。
*   **歧义分布**：
    *   大多数兼类词只有2个标记（如名词/动词）。
    *   少数词有更多标记（如 `still` 等），甚至多达7个，但频次极低。
    *   许多兼类很容易消除（如名词/动词兼类，根据位置很容易判断）。

## 三、 词类标注的主要方法

### 1. 基于规则的方法（Rule-Based Tagging）
*   **基本流程**：
    1.  **词典查寻**：给每个单词分配所有可能的词类标记。
    2.  **规则消歧**：利用手工编写的规则（消歧规则）去除不正确的标记。
*   **案例：ENGCG 系统 (English Constraint Grammar)**
    *   **词典信息丰富**：不仅包含基本词类，还包含句法特征（如主格、宾格）、配价信息（如 SVO, SVOO）等。
    *   **规则示例**：
        *   **去除 `that` 的副词标记**：如果 `that` 的下一个词是形容词/副词/量词，且 `that` 前一个词不是像 `consider` 这样的特定动词，则保留副词标记；否则去除。
    *   **知识来源**：
        *   内省知识（语感）。
        *   权威语法书（如 Quirk 的《英语语法大全》）。
        *   语料库诱导（通过观察语料库实例发现规则）。
*   **消歧策略**：
    *   **利用形态知识**：如 `-ed` 后缀通常暗示动词或分词。
    *   **利用上下文环境**：如限定词后不太可能是动词；`very` 后通常是形容词/副词。
    *   **利用语义与配价**：如 `buy` 后接表示物品的名词。
*   **优缺点**：
    *   优点：准确率较高（可达80%左右），规则具有可解释性。
    *   缺点：规则编写耗时费力，难以覆盖所有语言现象，无法做到100%正确。

### 2. 基于统计的方法（Stochastic/Probabilistic Tagging）
*   **基本思想**：不通过规则硬性判断，而是计算不同标记序列的**概率**，选择概率最大的序列。
*   **核心模型：隐马尔可夫模型（Hidden Markov Model, HMM）**
    *   **直觉**：
        *   考虑单词本身的生成概率（发射概率）。
        *   考虑前一个标记对当前标记的影响（转移概率）。
    *   **公式概念**：
        *   计算 $P(Word|Tag) \times P(Tag|Previous\ Tag)$。
        *   即：**（当前标记生成当前词的概率）×（前一个标记转移到当前标记的概率）**。
    *   **特点**：
        *   标记是“隐藏”的状态（Hidden）。
        *   单词是可观察的输出（Observation）。
        *   是一个马尔可夫过程（当前状态只依赖于前一个状态）。
    *   **优势**：无需人工编写大量复杂规则，通过数据训练即可。

### 3. 基于转换的方法（Transformation-Based Tagging / Brill Tagging）
*   **提出者**：Eric Brill。
*   **基本思想**：
    1.  **初始标注**：先给每个词分配最常见的标记（如全部标为名词）。
    2.  **错误驱动（Error-driven）**：发现错误后，利用规则进行修正（转换）。
    3.  **规则学习**：规则不是人写的，而是机器从语料库中自动学习出来的（如“如果前一个词是限定词，将名词标记改为动词”）。
*   **评价**：结合了规则方法的可解释性和统计方法的自动学习能力。

## 四、 老师的经验与观点
*   **工程经验**：
    *   老师曾为 NEC 开发英日机器翻译系统，编写了数百条规则，准确率达到70%-80%。
    *   基于规则的方法在达到一定准确率后会遇到瓶颈（长尾问题），剩下的错误通常需要在句法分析或语义分析阶段解决。
*   **学术建议**：
    *   **重视基础**：博士生入学考试中关于“乔姆斯基”或“萨丕尔”的基本常识题常有人答不出，说明基础不牢。
    *   **知识获取**：做研究不仅要靠语感，还要善于查阅权威语法书（如 Quirk）和词典。
    *   **方法融合**：虽然统计方法（如 HMM）现在很流行且效果好，但基于规则的方法依然有价值，两者可以互补。

以下是根据提供的课堂录音文稿（文件46和文件45）整理的计算语言学课程知识要点笔记。这两部分内容主要集中在**词性标注（Part-of-Speech Tagging）**的几种主要方法：统计方法（HMM）、基于规则的方法以及基于转换的方法（TBL）。

---

# 计算语言学课程笔记：词性标注方法

## 一、 隐马尔可夫模型（Hidden Markov Model, HMM）在词性标注中的应用

老师详细讲解了基于统计的HMM方法在词性标注中的数学原理和计算过程。

### 1. 核心思想
HMM 是一种**生成式模型**，它假设词性序列是隐藏的状态序列，而单词序列是观察到的输出。要为一个单词 $W_i$ 标注词性 $T_i$，需要考虑两个核心因素（存起来并取乘积最大值）：
1.  **转移概率（Transition Probability）**：前一个标记 $T_{i-1}$ 出现的情况下，当前标记 $T_i$ 出现的概率。即考虑上下文（Context）。
2.  **发射概率（Emission/Output Probability）**：在当前标记 $T_i$ 的情况下，生成该单词 $W_i$ 的概率。即考虑单词本身的词性倾向。

### 2. 数学公式推导
*   **目标**：找到一个标记序列 $T$，使得在给定单词序列 $W$ 的情况下，$P(T|W)$ 最大。
*   **贝叶斯公式展开**：
    $$P(T|W) = \frac{P(T) \cdot P(W|T)}{P(W)}$$
*   **简化**：由于对于同一个句子，$P(W)$ 是常数，因此只需要最大化分子：
    $$\hat{T} = \operatorname*{argmax}_T P(T) \cdot P(W|T)$$
*   **分项计算**：
    *   **$P(T)$（标记的概率）**：利用 N-gram 语法近似。例如使用 Bigram（二元模型），即 $P(T_i | T_{i-1})$。
    *   **$P(W|T)$（单词在给定标记下的概率）**：假设单词只与当前标记有关，即 $P(W_i | T_i)$。
*   **最终计算公式（Bigram近似）**：
    $$Score(T_i) \approx P(T_i | T_{i-1}) \cdot P(W_i | T_i)$$
    计算所有可能的标记序列的概率乘积，取最大值对应的序列作为结果（隐含了维特比算法的思想）。

### 3. 参数估算（从语料库中统计）
这些概率参数可以通过对标注好的语料库进行统计得到：
*   **转移概率计算**：
    $$P(T_i | T_{i-1}) \approx \frac{Count(T_{i-1}, T_i)}{Count(T_{i-1})}$$
    （即：前一个标记和当前标记同时出现的次数 / 前一个标记出现的总次数）
*   **发射概率计算**：
    $$P(W_i | T_i) \approx \frac{Count(W_i, T_i)}{Count(T_i)}$$
    （即：该单词被标记为该词性的次数 / 该词性出现的总次数）
*   **平滑处理（Smoothing）**：为了避免零概率问题（Zero Probability），实际计算中需要进行数据平滑。

### 4. 案例分析：兼类词消歧
老师通过具体的句子演示了如何利用上述两个因素解决歧义。
*   **案例 1**："Secretary is expected to **race** tomorrow"
    *   问题：`race` 是动词（VB）还是名词（NN）？
    *   分析：
        *   看前面：`to` 后面接动词（VB）的概率 $P(VB|TO)$ 远大于接名词（NN）的概率 $P(NN|TO)$。
        *   看本身：`race` 作为动词和名词的自身概率。
        *   综合：两者乘积，判断此处为动词。
*   **案例 2**："The reason for the **race** for outer space"
    *   问题：这里的 `race` 是什么词性？
    *   分析：前面是 `the`，`the` 后面接名词（NN）的概率极高。综合计算后，判断此处为名词。

---

## 二、 基于转换的学习方法（Transformation-Based Learning, TBL）

老师介绍了由 **Eric Brill** 提出的基于转换的错误驱动学习方法（也称 Brill Tagger）。这是一种结合了规则与统计的方法。

### 1. 形象比喻：画画（Painting Analogy）
为了解释 TBL，老师引用了一个著名的比喻：画一幅“蓝天背景下的白房子，房子有绿窗户”。
1.  **第一步（初始状态）**：把整张画布全涂成蓝色（因为大部分面积是蓝天）。—— *对应：先按最频繁的词性给所有词标注。*
2.  **第二步（修正1）**：把房子区域改成白色。—— *对应：应用规则修正部分标记。*
3.  **第三步（修正2）**：把白房子上的窗户区域改成绿色。—— *对应：应用更细致的规则修正。*
4.  **第四步（修正3）**：如果屋顶是棕色的，再加一笔。
*   **核心逻辑**：先大面积覆盖（由统计决定初始状态），再通过一系列规则逐步修正错误。

### 2. 算法流程
1.  **初始标注**：利用未标注文本，根据字典中最可能的标记（Unigram probability）给每个词一个初始词性。
2.  **学习规则**：对比当前标注与正确标注（真值），自动学习出能减少最多错误的转换规则。
3.  **应用规则**：按顺序应用这些规则进行修正。

---

## 三、 基于规则的消歧方法（Rule-Based Approach）

在 HMM 和 TBL 之外，老师还补充了传统的基于规则的方法，强调了语言学知识的作用。

### 1. 知识来源
*   **直觉（Introspection）**：基于语言学家的语感和内省（Linguistic Intuition）。
*   **归纳（Induction）**：通过观察语料库（如 KWIC 索引），发现规律。
*   **标准数据**：查阅词典或参考书。

### 2. 规则类型与特征
*   **形态学线索（Morphology）**：
    *   词尾 `-ed`：可能是过去分词（VBN）或过去式（VBD）。
    *   词尾 `-ing`：可能是动名词或进行时（VBG）。
*   **句法模板（Syntactic Templates）**：
    *   `N + V + N` 结构。
    *   `Det + Adj + N` 结构。
*   **语义限制（Semantic Constraints）**：
    *   例如 `book`，如果是 "book a room"，后面跟“房间”，则 `book` 是动词（预订）；如果是 "read a book"，则 `book` 是名词。

### 3. 具体案例："That" 的词性判断
单词 `that` 歧义很大，可以是限定词、关系代词、连词或副词。
*   **副词判别规则（Adverbial 'that'）**：
    *   规则条件：如果 `that` 后面紧跟形容词（Adj）、副词或量词。
    *   否定证据：如果前面没有特定动词结构。
    *   例子："It is not **that** bad"（那没那么坏）。这里的 `that` 修饰形容词 `bad`，是副词（相当于 `so`）。
    *   老师提到参考了 **Constraint Grammar (Karlsson)** 的思路，编写消歧规则。

---

## 四、 混合策略与历史发展

### 1. 混合策略（Hybrid Strategy）
*   由于纯规则方法很难覆盖 100% 的语言现象，老师提出一种实用策略：
    *   利用**显性规则（Explicit Rules）**处理 80% 明显的问题（大面积消歧）。
    *   剩下的 20% 疑难杂症，通过**统计模型**或更深层的语义分析来解决。
    *   这种方法在工程实践中（如机器翻译系统）效果较好且效率高。

### 2. 统计方法的历史沿革
*   老师简要回顾了统计方法在计算语言学中的复兴：
    *   早期：Bahl, Jelinek, Mercer (IBM 团队) 在语音识别中引入统计方法。
    *   发展：Church (Church parts of speech program), DeRose 等人将 HMM 应用于词性标注。
    *   对比：
        *   **规则方法**：问“这是什么词？符合什么规则？”
        *   **统计方法**：问“在给定的个候选标记中，哪一个计算出的概率乘积最大？”

## 五、 课堂总结
*   **词性标注的三大流派**：
    1.  **基于规则（Rule-based）**：依赖语言学知识，编写规则（如 Constraint Grammar）。
    2.  **基于统计（Statistical / HMM）**：依赖语料库，计算转移概率和发射概率。
    3.  **基于转换（Transformation-based / TBL）**：结合两者，先统计初始化，后规则修正（Brill Tagger）。
*   **HMM 核心**：利用贝叶斯公式，权衡“上下文转移”和“单词自身发射”两个概率。

以下是根据提供的录音文稿（文件48和文件47）整理的计算语言学课程知识要点笔记。这两部分主要深入讲解了词性标注（POS Tagging）中的隐马尔可夫模型（HMM）细节、未知词处理、基于转换的学习（TBL）以及文学中的序列结构案例。

# 计算语言学课程笔记：词性标注进阶与模型

## 一、 隐马尔可夫模型（HMM）在词性标注中的计算细节

### 1. 核心概率计算
HMM模型在词性标注中主要计算两个概率的乘积，以找出最大可能性的标记序列（Viterbi算法）：

*   **转移概率 (Transition Probability)**：
    *   **概念**：在已知前一个（或前几个）标记的情况下，当前词出现某个标记的概率。
    *   **公式逻辑**（以三元模型Trigram为例）：
        $$P(t_n | t_{n-2}, t_{n-1}) \approx \frac{\text{Count}(t_{n-2}, t_{n-1}, t_n)}{\text{Count}(t_{n-2}, t_{n-1})}$$
    *   **解释**：计算 $t_{n-2}, t_{n-1}, t_n$ 三个标记同时出现的次数，除以 $t_{n-2}, t_{n-1}$ 两个标记同时出现的次数。

*   **发射概率 (Emission Probability / Observation Likelihood)**：
    *   **概念**：在已知词性标记 $t_n$ 的情况下，生成具体单词 $w_n$ 的概率。
    *   **公式逻辑**：
        $$P(w_n | t_n) = \frac{\text{Count}(w_n \text{ given } t_n)}{\text{Count}(t_n)}$$
    *   **解释**：所有被标记为 $t_n$ 的词的总次数作为分母，单词 $w_n$ 被标记为 $t_n$ 的次数作为分子。
    *   **案例**：单词 "race" 既可以是动词也可以是名词。需要计算在语料库中 "race" 被标为动词的概率。

### 2. HMM 标注案例解析 ("to race")
*   **情境**：区分 "expected to race tomorrow" 和 "the race for outer space"。
*   **计算过程**：
    *   虽然 "race" 作为名词（NN）的先验概率（Unigram）可能高达0.92，作为动词（VB）很低。
    *   但是，HMM 考虑上下文（转移概率）。当 "to" 出现时：
        *   $P(NN | to)$ （TO后面接名词）的概率很低。
        *   $P(VB | to)$ （TO后面接原形动词）的概率很高。
    *   **结论**：将转移概率与发射概率相乘，"to race" 中 "race" 标记为 VB 的总概率会远高于 NN，从而实现正确标注。

## 二、 词性标注中的难点与处理技巧

### 1. 多重标记与兼类词 (Multi-part/Category Words)
*   **问题**：语言学上难以定性的词。
    *   **中文案例**：“这本书的**出版**”。有“的”看似名词，但又能受副词修饰（“迟迟不出版”）看似动词。
    *   **英文案例**：过去分词与形容词的界限（如 words ending in -ed），名词与动词兼类（如 light）。
*   **对策**：
    *   在无法确定的情况下，允许**多重标记**（Giving multiple tags）。
    *   Penn Treebank 等语料库中存在此类处理，承认语言的模糊性。

### 2. 短语与多词表达 (Multi-word Expressions)
*   **习语处理**：如 "in terms of"。
    *   处理方式：视为一个整体，但在标注时分别标记为 IN (Preposition)，并用下标区分（如 IN1, IN2, IN3），表示它们属于同一个介词短语结构。
*   **缩略词拆分**：如 "won't"。
    *   处理方式：拆分为 "wo" (will/MD) 和 "n't" (not/RB) 分别标注。

### 3. 未登录词/未知词处理 (Unknown Words / OOV)
新闻语料中常出现人名、机构名等未在词典中的词，处理方法如下：
*   **Hapax Legomena 方法**：
    *   利用语料库中**只出现一次的词（Hapax Legomena）**的分布特征，作为未知词的概率分布模型。
*   **利用拼写/形态学特征 (Morphological Features)**：
    *   **大写字母**：首字母大写可能为专有名词（NP）。
    *   **后缀判断**：
        *   *-ed* $\rightarrow$ 过去式/过去分词
        *   *-s* $\rightarrow$ 名词复数/动词第三人称单数
        *   *-ing* $\rightarrow$ 动名词/进行时
        *   *-ion* $\rightarrow$ 名词
        *   *-al, -ive* $\rightarrow$ 形容词
        *   *-ly* $\rightarrow$ 副词
    *   **连字符 (Hyphen)**：带有连字符的词有特定的词性分布。
*   **具体算法**：计算特定特征（如大写、后缀）对应的标记概率分布 $P(t_i | \text{feature})$，用于估算未知词的 $P(t_i | w_{unknown})$。

## 三、 基于转换的学习 (Transformation-Based Learning, TBL)
老师介绍了 Brill Tagger（Eric Brill提出）的原理，这是一种结合了统计和规则的方法。

### 1. 基本流程
1.  **初始标注**：根据统计（Unigram），将所有词标注为该词在语料库中最常见的标记（例如，将所有的 "race" 都先标为 NN）。
2.  **错误驱动的修正 (Error-Driven)**：
    *   对比当前标注与正确标注（黄金标准）。
    *   应用规则进行修正（例如：将 NN 改为 VB）。
3.  **规则学习**：系统自动学习能够最大程度减少错误的规则。

### 2. 模板 (Templates) 的使用
为了避免规则过于具体（过拟合）或难以覆盖，使用**模板**来生成规则：
*   **模板示例**：
    *   "Change tag A to tag B if the previous tag is Z."
    *   "Change tag A to tag B if one of the two preceding words is tagged Z."
*   **规则实例**：
    *   **Change NN to VB if previous tag is TO.** (处理 "to race")
    *   **Change VBP to VB if one of previous 3 tags is MD.** (处理 "might vanish"，vanish 不应是VBP而是VB)

## 四、 跨学科案例：文学中的马尔可夫链

老师以普希金（Pushkin）的长篇诗体小说《叶甫盖尼·奥涅金》（Eugene Onegin）为例，说明序列和结构在语言中的存在（类似马尔可夫链）。

*   **奥涅金诗节 (Onegin Stanza)**：由14行诗句组成，具有严格的韵律结构。
*   **韵律模式解析**：
    1.  **交叉韵 (Cross rhyme)**：ABAB （第1-4行）
    2.  **重叠韵/随韵 (Couplet/Pair rhyme)**：CCDD （第5-8行）
    3.  **环抱韵 (Enclosed rhyme)**：EFFE （第9-12行）
    4.  **结尾对韵**：GG （第13-14行）
*   **隐喻**：这种严格的排列组合和前后依赖关系，在数学形式上与马尔可夫链（Markov Chain）中的状态转移有异曲同工之妙。

## 五、 总结
*   **HMM 方法**：属于生成式模型，通过计算 $P(Word|Tag) \times P(Tag|Previous Tag)$ 寻找全局最优路径。
*   **TBL 方法**：属于判别式/修正式模型，先统计后规则，通过学习规则模板来修正错误。
*   **展望**：无论是词性标注、语音识别还是机器翻译，本质上都是从观察到的符号（Observed）推断隐藏的结构（Hidden），这是计算语言学的核心问题。


以下是根据课堂录音文稿（49-50）整理的计算语言学课程知识要点笔记：

# 计算语言学（Computational Linguistics）课程笔记 - 续

## 六、 马尔可夫模型的起源与发展

### 1. 起源：文学与数学的结合
*   **背景**：普希金的长篇叙事诗《叶甫盖尼·奥涅金》（Eugene Onegin）。
*   **文学视角**：文学家关注其特殊的韵律结构（如ABABCCDD），称为“奥涅金诗节”（Onegin stanza）。
*   **数学视角（马尔可夫 Andrey Markov）**：
    *   **研究对象**：不关注韵律美感，而是关注**元音（Vowel）与辅音（Consonant）之间的序列关系**。
    *   **发现**：前面的字母可以预测后面字母出现的属性及其概率。字母间的序列关系并非杂乱无章，而是存在明确的统计规律。
    *   **意义**：将文本序列看作**随机过程（Stochastic Process）**。这一思想使得马尔可夫过程（Markov Process）和马尔可夫链（Markov Chain）成为普遍的数学方法。

### 2. 信息论的诞生（香农 Shannon）
*   **应用**：香农在20世纪50年代利用马尔可夫链计算英语字母的信息量（Entropy）。
*   **成果**：
    *   计算出英语字母的信息熵大约为3.x比特。
    *   **定义了“比特”（Bit）**：作为衡量信息的单位（基于二进制，两种对等可能性时的判断难度）。
    *   **评价**：香农被称为“信息论之父”，其理论奠定了整个信息社会可计算性的基础。

## 七、 显性马尔可夫模型 (Markov Chain / Visible Markov Model)

### 1. 定义
*   实质是一个**加权有限状态自动机（Weighted Finite State Automaton）**。
*   **特点**：状态（State）是显性的，可以直接观察到。

### 2. 构成要素
*   **状态（Q）**：$Q_1, Q_2, \dots, Q_N$。
*   **转移概率（A）**：$a_{ij}$ 表示从状态 $i$ 转移到状态 $j$ 的概率。
    *   约束条件：从任一状态出发，所有转移概率之和为1 ($\sum a_{ij} = 1$)。
*   **初始状态（$Q_0$ / $\pi$）**与**终结状态（$Q_F$）**。

### 3. 马尔可夫假设（Markov Assumption）
*   为了简化计算，假设一个特定状态的概率**只与它前一个状态有关**（一阶马尔可夫链）。
    *   虽然现实中可能与前$N$个状态都有关，但模型中简化为：$P(q_i | q_{i-1}, q_{i-2} \dots) \approx P(q_i | q_{i-1})$。

### 4. 案例：天气模型（显性）
*   **状态**：Hot, Cold, Warm。
*   **计算**：如果已知状态转移概率（如 Hot->Hot=0.5, Hot->Cold=0.2 等），可以计算特定序列（如 "Hot Hot Hot" 或 "Cold Hot Cold"）出现的概率。

## 八、 隐马尔可夫模型 (Hidden Markov Model, HMM)

### 1. 概念引入
*   **为什么需要“隐”模型？**
    *   在很多实际问题中，感兴趣的事件（状态）无法直接观察，只能通过表面现象（观察值）来推断。
    *   **词性标注（POS Tagging）**：观察到的是单词（Observed），隐藏的是词性（Hidden）。
    *   **语音识别**：观察到的是声学信号，隐藏的是单词。

### 2. 教学案例：杰森·艾斯纳（Jason Eisner）的冰淇淋与天气
*   **参考资料**：*On Interactive Spreadsheet for Teaching the Forward-Backward Algorithm* (Jason Eisner)。
*   **场景设定**：
    *   **任务**：根据某人（Jason）每天吃冰淇淋的数量，推断当天的天气（Hot 或 Cold）。
    *   **可见（观察值 Observation, O）**：冰淇淋数量（1个、2个或3个）。
    *   **不可见（状态 State, Q）**：天气（H, C）。

### 3. HMM的形式化定义
隐马尔可夫模型比显性模型更复杂，包含五个要素（五元组）：
1.  **状态集合 ($Q$)**：$Q_1, \dots, Q_N$（如 Hot, Cold）。
2.  **转移概率矩阵 ($A$)**：状态之间的转移概率（如 $P(Hot|Cold)$）。
3.  **观察序列 ($O$)**：$O_1, O_2, \dots, O_T$（如 1, 2, 3个冰淇淋）。
4.  **初始状态分布 ($\pi$)**：模型启动时各状态的概率。
5.  **发射概率/观察似然度 (Emission Probabilities / B)**：
    *   **定义**：在特定状态下，生成某个观察值的概率。
    *   表示为 $b_i(o_t)$：从状态 $i$ 生成观察 $o_t$ 的概率。
    *   *例如：在Hot天气下吃3个冰淇淋的概率。*

### 4. HMM的两个基本假设
为了使计算可行，HMM对现实世界进行了极度简化（类比物理学中的理想气体方程 PV=nRT）：

1.  **马尔可夫假设 (Markov Assumption)**：
    *   当前状态只取决于前一个状态 ($t-1$)，与更早的状态无关。
2.  **输出独立性假设 (Output Independence Assumption)**：
    *   当前的观察值（输出）**只取决于当前的状态**，与其他观察值或状态无关。
    *   *例子*：虽然现实中吃冰淇淋的数量可能受前一天吃多了的影响，或者受周围环境影响，但在模型中，我们假设它只受当天天气（当前状态）影响。

### 5. 案例具体参数（冰淇淋模型）
根据课堂演示的图示参数：

*   **状态转移概率 ($A$)**：
    *   Start -> Hot: 0.8
    *   Start -> Cold: 0.2
    *   Hot -> Hot: 0.7
    *   Hot -> Cold: 0.3
    *   Cold -> Cold: 0.6
    *   Cold -> Hot: 0.4
*   **发射概率 ($B$)** (即某天气下吃某数量冰淇淋的概率)：
    *   **Hot 状态下**：
        *   1个冰淇淋: 0.2
        *   2个冰淇淋: 0.4
        *   3个冰淇淋: 0.4
    *   **Cold 状态下**：
        *   1个冰淇淋: 0.5
        *   2个冰淇淋: 0.4
        *   3个冰淇淋: 0.1
*   **逻辑**：天热吃多的概率大（3个占0.4），天冷吃少的概率大（1个占0.5）。

## 九、 课程后续预告
*   接下来的课程将基于上述 HMM 模型及参数，讲解相关的算法（如最大熵模型 Maximum Entropy、最大熵马尔可夫模型 MEMM）。
*   HMM 本质上是一种**序列标注（Sequence Labeling）**工具，属于分类器的一种。



以下是根据课堂录音文稿（五十一、五十二）整理的计算语言学课程知识要点笔记：

# 计算语言学课程笔记：隐马尔可夫模型（HMM）进阶

## 一、 案例回顾与模型参数设定
老师继续通过“吃冰淇淋（观察值）推测天气（隐藏状态）”的案例来讲解HMM的计算。

### 1. 案例数据设定
*   **观察序列 (Observations, O)**：3, 1, 3（第一天吃3根，第二天吃1根，第三天吃3根）。
*   **隐藏状态 (Hidden States, Q)**：
    *   Hot (热)
    *   Cold (冷)
*   **模型参数 ($\lambda$)**：
    *   **初始概率 ($\pi$)**：
        *   Start -> Hot: 0.8
        *   Start -> Cold: 0.2
    *   **状态转移概率 (Transition Probabilities, A)**：
        *   Hot -> Hot: 0.7
        *   Hot -> Cold: 0.3
        *   Cold -> Cold: 0.6
        *   Cold -> Hot: 0.4
    *   **发射概率/观察概率 (Emission Probabilities, B)**：
        *   **Hot状态下**：吃1根(0.2), 吃2根(0.4), 吃3根(0.4)
        *   **Cold状态下**：吃1根(0.5), 吃2根(0.4), 吃3根(0.1)

### 2. 模型拓扑结构
*   **全连通模型 (Ergodic Model)**：
    *   任何状态之间都可以互相转换（如本案例中的天气模型）。
*   **左右模型 (Left-to-Right / Bakis Model)**：
    *   状态只能保持当前或向后转移（如1->1, 1->2），不能倒退。
    *   **应用**：在**语音识别 (Speech Recognition)** 中非常有用，因为语音随时间单向流动。

## 二、 隐马尔可夫模型的三个基本问题
引用自 **Lawrence Rabiner (1989)** 的经典文献 *《A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition》*。所有关于HMM的研究基本围绕这三个问题展开：

1.  **似然度/评估问题 (The Likelihood/Evaluation Problem)**：
    *   给定模型 $\lambda=(A, B)$ 和观察序列 $O$，计算该模型产生该观察序列的概率 $P(O|\lambda)$ 是多少？
    *   **解决方法**：向前算法 (Forward Algorithm)。
2.  **解码问题 (The Decoding Problem)**：
    *   给定模型 $\lambda$ 和观察序列 $O$，找到最有可能的隐藏状态序列 $Q$（即解释观察结果的最佳状态路径）。
    *   **解决方法**：维特比算法 (Viterbi Algorithm)。
3.  **学习问题 (The Learning Problem)**：
    *   给定观察序列 $O$ 和状态集合，如何调整模型参数（$A$和$B$），使得 $P(O|\lambda)$ 最大化。
    *   **解决方法**：向前向后算法 (Forward-Backward Algorithm) / Baum-Welch算法（本课程因时间限制不详细展开）。

## 三、 问题一详解：观察序列似然度的计算

### 1. 穷举法 (Direct/Brute-Force Approach)
*   **思路**：列举所有可能的隐藏状态序列，计算每一种序列产生观察值的概率，然后求和。
*   **计算公式**：$P(O|\lambda) = \sum_{Q} P(O,Q|\lambda) = \sum_{Q} P(O|Q,\lambda)P(Q|\lambda)$
*   **案例应用**：
    *   观察为3-1-3，状态只有Hot/Cold。
    *   可能的状态序列有 $2^3 = 8$ 种（如 HHH, HHC, HCH, HCC...）。
    *   以 **Hot-Hot-Cold** 为例的计算：
        1.  **路径概率**：$P(Start \to H) \times P(H \to H) \times P(H \to C) = 0.8 \times 0.7 \times 0.3$
        2.  **发射概率**：$P(3|H) \times P(1|H) \times P(3|C) = 0.4 \times 0.2 \times 0.1$
        3.  两者相乘得到该路径的联合概率。
*   **局限性**：
    *   **计算复杂度 (Complexity)**：$O(N^T)$。其中 $N$ 是状态数，$T$ 是观察序列长度。
    *   **问题**：呈指数级增长。在词性标注等实际应用中（如$N=3$个词性，$T=50$个词），计算量巨大（$3^{50}$），实际上无法计算。

### 2. 向前算法 (Forward Algorithm)
为了解决穷举法的复杂度问题，引入动态规划思想。

*   **核心概念**：**向前变量 $\alpha_t(j)$ (Forward Variable)**
    *   **定义**：在时间步 $t$，观察到序列 $O_1, O_2, ..., O_t$，且当前位于状态 $j$ 的概率。
*   **数据结构**：**向前网格 (Forward Trellis / Forward Grid)**
    *   **横轴**：时间/观察序列 (Time Steps: $t=1, 2, ...$)
    *   **纵轴**：状态空间 (States: $Q_1, Q_2, ...$)
    *   **优势**：将时间与空间结合，避免重复计算。

#### 算法步骤与递归公式：
1.  **初始化 (Initialization)**：计算 $t=1$ 时的 $\alpha$ 值。
    *   $\alpha_1(j) = \pi_j \times b_j(O_1)$
    *   即：(初始转移概率) $\times$ (在状态 $j$ 发射 $O_1$ 的概率)
2.  **递归/归纳 (Induction)**：计算 $t+1$ 时的值。
    *   $\alpha_t(j) = [\sum_{i=1}^{N} \alpha_{t-1}(i) \times a_{ij}] \times b_j(O_t)$
    *   **含义**：
        1.  **汇聚前一步**：将前一时刻 $t-1$ 所有可能状态 $i$ 的向前概率 $\alpha_{t-1}(i)$，乘以从 $i$ 转移到当前状态 $j$ 的概率 $a_{ij}$，并求和。
        2.  **当前发射**：再乘以当前状态 $j$ 发射观察值 $O_t$ 的概率 $b_j(O_t)$。
    *   **形象描述**：“老是向前走”，利用前一步的结果算后一步。
3.  **终结 (Termination)**：
    *   将最后时刻 $T$ 所有状态的 $\alpha_T(i)$ 相加，即得到整个观察序列的概率 $P(O|\lambda)$。

#### 案例计算演示 (3-1-3序列)：
*   **Time 1 (观察值=3)**：
    *   $\alpha_1(Cold) = 0.2 (Start \to C) \times 0.1 (3|C) = 0.02$
    *   $\alpha_1(Hot) = 0.8 (Start \to H) \times 0.4 (3|H) = 0.32$
*   **Time 2 (观察值=1)**：
    *   计算 $\alpha_2(Cold)$：需考虑从 $t=1$ 的 Hot 和 Cold 转移过来的两种路径。
        *   路径1 (C->C): $0.02 \times 0.6 \times 0.5$
        *   路径2 (H->C): $0.32 \times 0.3 \times 0.5$
        *   求和得到 $\alpha_2(Cold)$。
    *   同理计算 $\alpha_2(Hot)$。
*   **Time 3 (观察值=3)**：
    *   基于 $\alpha_2$ 的结果继续递归。

### 3. 复杂度对比
*   **穷举法**：$2T \cdot N^T$ （指数级）
*   **向前算法**：$N^2T$ （多项式级）
    *   将计算复杂度从指数级降低到了乘法级，使得长序列的计算成为可能。

## 四、 问题二：解码问题 (Decoding)
*   **背景**：
    *   向前算法只能告诉我们“这个观察序列出现的概率有多大”，通过求和掩盖了具体的路径信息。
    *   在词性标注等任务中，我们需要知道具体的“正确答案”（即每个词对应的具体词性/状态是什么）。
    *   仅仅找出概率最大的单个状态点是不够的，因为状态之间有转移限制（不能只看局部最优）。
*   **任务目标**：
    *   给定模型 $\lambda$ 和观察序列 $O$，发现最优的隐藏状态序列 $Q$。
*   **解决方法**：**维特比算法 (Viterbi Algorithm)**
    *   老师预告这是下一讲的重点。
    *   Viterbi 也是一种动态规划算法，用于寻找全局最优路径。

## 五、 重点词汇与概念
*   **HMM (Hidden Markov Model)**：隐马尔可夫模型
*   **Transition Probability (A)**：转移概率
*   **Emission Probability (B)**：发射概率
*   **Forward Algorithm**：向前算法
*   **Forward Trellis**：向前网格
*   **Complexity**：复杂度
*   **Likelihood**：似然度
*   **Decoding**：解码



以下是根据课堂录音文稿（53-54）整理的计算语言学课程知识要点笔记：

# 计算语言学课程笔记（五十三、五十四）

## 一、 维特比算法 (Viterbi Algorithm) 详解

维特比算法是一种动态规划算法，用于寻找隐马尔可夫模型（HMM）中的**最佳路径**（即最可能的隐藏状态序列）。

### 1. 算法核心机制
*   **网格结构 (Lattice/Grid)**：
    *   **横轴**：时间步（Time steps），对应观察序列（$O_1, O_2, O_3...$），如：吃冰淇淋的数量（3个、1个、3个）。
    *   **纵轴**：状态空间（State space），对应隐藏状态（$Q_0, Q_1, Q_2...$），如：天气状态（Start, Hot, Cold）。
*   **计算逻辑**：
    *   **递归与最大化**：不同于向前算法（Forward Algorithm）将所有路径概率相加，维特比算法在每一步都**选取概率最大**的那条路径保留下来，舍弃较小的路径。
    *   **目的**：降低计算复杂性，保证每一步得到局部最优，最终通过回溯得到全局最优。

### 2. 计算公式与要素
维特比变量 $V_t(j)$ 表示在时间 $t$ 处于状态 $j$ 的最佳路径概率：
$$V_t(j) = \max_{i} [V_{t-1}(i) \times a_{ij} \times b_j(o_t)]$$
其中包含三个因子：
1.  **$V_{t-1}(i)$**：前一时刻（$t-1$）在状态 $i$ 的维特比路径概率。
2.  **$a_{ij}$**：转移概率（Transition Probability），从状态 $i$ 转移到状态 $j$ 的概率。
3.  **$b_j(o_t)$**：发射概率（Emission Probability），在状态 $j$ 下观察到符号 $o_t$ 的概率。

### 3. 反向指针与回溯 (Backpointer & Backtracking)
*   **必要性**：仅计算出最大概率值是不够的，HMM的目标是解码出状态序列。
*   **操作**：
    *   在计算每一步的 $V_t(j)$ 时，记录产生该最大值的来源状态（即前一个最佳状态是谁）。
    *   **反向追踪 (Backtracking)**：当计算到最后一个时间步，找到最大概率的状态后，利用记录的反向指针，从后往前（End $\to$ Start）一步步回溯，从而确认整个最佳路径序列（如：Start $\to$ Hot $\to$ Cold $\to$ Hot）。
*   **双重确认**：这是一种保险机制，确保解码结果的正确性。

### 4. 维特比算法 vs 向前算法 (Forward Algorithm)
| 特征 | 向前算法 (Forward) | 维特比算法 (Viterbi) |
| :--- | :--- | :--- |
| **计算方式** | 求和 ($\sum$) | 取最大值 ($\max$) |
| **目标** | 计算观察序列出现的总概率 | 寻找最可能的隐藏状态序列（解码） |
| **回溯** | 不需要 | **需要** (Backpointer) |

---

## 二、 科学研究的方法论与哲学
老师通过HMM和维特比算法引申出科学研究的哲学思考：
*   **透过现象看本质**：
    *   HMM 的核心是从**表面现象**（观察值，如吃冰淇淋）推测**背后实质**（隐藏状态，如天气）。
    *   科学研究的任务就是通过观察到的表象，发现背后的规律。
*   **爱因斯坦的观点**：
    *   人类容易被表面现象（经验）迷惑（例如认为太阳围着地球转）。
    *   当观测事实与经过深思熟虑的严密理论（Logic/Theory）矛盾时，不要急于修正理论，而要怀疑观察方法或事实本身可能存在偏差。
    *   **警示**：不要轻易因为眼前的数据否定理论，要学会检查观察手段是否正确。

---

## 三、 HMM 在词类标注 (POS Tagging) 中的应用

### 1. 问题定义
*   **任务**：给定一个句子（单词序列），为每个单词指派最可能的词性标记（Tag）。
*   **本质**：分类问题，通过 HMM 寻找产生该单词序列概率最大的标记序列。

### 2. 标注计算逻辑
对于单词 $w_i$ 选择标记 $t_i$，使其概率最大化。计算涉及两部分：
1.  **转移概率 ($P(t_i | t_{i-1})$)**：前一个标记 $t_{i-1}$ 转移到当前标记 $t_i$ 的概率（语法规则的体现）。
2.  **发射概率 ($P(w_i | t_i)$)**：当前标记 $t_i$ 生成单词 $w_i$ 的概率（词汇生成的体现）。

### 3. 案例分析： "The race" vs "to race"
*   **句子片段**：`... expected to race tomorrow`
*   **歧义词**：`race` 既可以是动词 (VB)，也可以是名词 (NN)。
*   **上下文线索**：前一个词是 `to`。
*   **计算比较**：
    *   **路径 A (动词)**：$P(VB|TO) \times P(race|VB)$
    *   **路径 B (名词)**：$P(NN|TO) \times P(race|NN)$
*   **结果**：虽然 `race` 作为名词的先验概率可能很高（80%是名词），但在 `to` 之后，转移概率 $P(VB|TO)$ 极高，使得路径 A 的总概率大于路径 B，因此标注为 **VB**。

### 4. 关于“发射概率”的特别说明
*   **方向性**：是 **State $\to$ Observation**（从状态到观察）。
*   **直觉误区**：不是计算“这个词是动词的概率是多少”（这是后验概率），而是计算“如果现在的状态是动词，它表现为单词 'race' 的概率是多少”。
*   **数值特征**：发射概率通常非常小（例如 0.00003），因为词汇表很大，某个特定词在所有动词中出现的概率很低。

---

## 四、 最大熵模型 (Maximum Entropy / MaxEnt)

### 1. 概述
*   **定义**：一种多元逻辑回归（Multinomial Logistic Regression）模型，属于指数分类器（Exponential Classifier）或对数线性模型（Log-linear Model）。
*   **核心思想**：在满足已知约束条件（特征）的前提下，选择熵（Entropy）最大的分布，即保留最大的不确定性，不做无根据的假设。

### 2. 分类 (Classification) 与 回归 (Regression)
*   **分类**：给出离散结果（Yes/No，或者类别 A/B/C）。
*   **回归**：给出实数值（Real value）。
*   **关系**：逻辑回归是一种特殊的分类方法，它通过计算归属于某一类的**实数概率值**来进行分类。

### 3. 模型构建：特征与权重
MaxEnt 模型通过**特征 (Features)** 和 **权重 (Weights)** 来工作：
*   **特征 ($f$)**：从输入 $x$ 中抽取的有用信息。
    *   *例子*：单词以 "-ing" 结尾；前一个词是 "the"；文本中包含特定关键词（如情感分析中的褒义/贬义词）。
*   **权重 ($w$)**：通过训练学得的参数，表示该特征对分类的重要性。
*   **计算公式**：
    $$P(c|x) = \frac{1}{Z} \exp \left( \sum_{i} w_i f_i(x, c) \right)$$
    *   $\exp$：指数函数（$e$ 的次方）。
    *   $\sum w_i f_i$：特征与权重的线性组合。
    *   $Z$：归一化因子（Normalization factor），确保所有类别的概率之和为 1。

### 4. 应用场景
*   **文本分类 (Text Classification)**：将文本归类为体育、政治、农业等。
*   **情感分析 (Sentiment Analysis)**：判断评论是正面还是负面。
*   **信息过滤 (Filtering)**：识别并过滤不良信息（如垃圾邮件、敏感政治内容等）。
    *   *案例*：通过识别特定关键词（如“真善忍”等）及其组合特征，计算文章属于某类特定内容的概率，从而进行过滤。

### 5. MaxEnt 与 HMM 的对比
*   **HMM (隐马尔可夫模型)**：
    *   **生成式模型**：关注**观察序列**与**隐藏状态**之间的生成和转移关系。
    *   *路径*：观察 $\to$ 寻找隐藏状态序列。
*   **MaxEnt (最大熵模型)**：
    *   **判别式模型**：关注**特征**对**类别**的预测能力。
    *   *路径*：抽取特征 $\to$ 加权求和 $\to$ 计算归属各类别的概率。
    *   *灵活性*：可以融合各种不同类型的特征（词汇、句法、位置等），不仅仅局限于前后状态。


以下是根据录音文稿整理的计算语言学课程知识要点笔记（续前）：

# 计算语言学课程笔记（续）

## 一、 最大熵模型（Maximum Entropy Model）引论

### 1. 模型背景与对比
*   **隐马尔可夫模型 (HMM)**：
    *   属于生成模型。
    *   基于观察到的数据，探测背后的状态。
    *   计算依赖：状态转移概率 + 状态到观察的发射概率。
*   **最大熵模型 (MaxEnt)**：
    *   统计计算中应用普遍的模型。
    *   属于**分类模型**（Discriminative/Probabilistic Classifier）。
    *   **核心机制**：基于**特征 (Features)** 和 **特征的加权 (Weights)**。
    *   与HMM的区别：HMM依赖概率转移，最大熵依赖特征线性组合。

### 2. 分类原理与案例
*   **基本思想**：提取观察对象的特征，给每个特征赋予权重，通过特征与权重的总和来进行分类。
*   **案例：单词 "Race" 的词性分类**
    *   **问题**：判断 "Race" 是动词 (Verb) 还是名词 (Noun)。
    *   **特征提取**：
        *   特征1：前一个词是 "to" $\rightarrow$ 可能是动词（但也可能是名词，如 "to school"）。
        *   特征2：前一个词是 "the" $\rightarrow$ 可能是名词。
    *   **决策**：不同特征有不同权重（权值），综合考虑所有特征及其可能性进行判断。

## 二、 信息论基础（Information Theory Basics）

老师花大量篇幅讲解了最大熵模型背后的理论基础——熵。

### 1. 信息的定义与度量
*   **交际过程**：发送者 $\rightarrow$ 接收者。
*   **信息 (Information)**：
    *   在接收信息前，接收者存在**不定度 (Uncertainty)**。
    *   接收信息后，不定度消除（归零）。
    *   **定义**：信息量等于被消除的不定度（熵）。
    *   **香农 (Shannon)**：美国数学家，著有《通信的数学理论》(Mathematical Theory of Communication)。提出用“熵”来度量信息量。

### 2. 熵 (Entropy) 的起源与公式推导
*   **Hartley (1928) 的贡献**：
    *   提出用对数 ($\log$) 来测量系统的信息能力。
    *   **理由**：如果系统状态数由 $N$ 变为 $N^2$ 或 $N^3$（如掷1颗骰子 vs 2颗骰子），其复杂程度应是加倍或三倍关系 ($2\log N, 3\log N$)，而非指数增长。对数运算符合这一直觉。
*   **香农公式**：
    *   采用以2为底的对数 ($\log_2$)。
    *   **单位**：比特 (Bit)。
    *   **情况A：等概率事件 (Equiprobable)**
        *   公式：$H_0 = \log_2 N$
        *   案例：掷硬币（2面等概率），$H = \log_2 2 = 1$ bit。
    *   **情况B：不等概率事件**
        *   公式：$H = -\sum p_i \log_2 p_i$
        *   结论：等概率时熵最大 ($H_0$)，不等概率时熵减小 ($H < H_0$)。

### 3. 条件熵与马尔可夫过程
*   **语言的序列性**：前一个符号会影响后一个符号出现的概率（马尔可夫链）。
*   随着考虑的上下文（前 $N$ 个词）增加，对下一个词的预测越准确，不定度（熵）越低。
*   **极限熵 (Limit Entropy, $H_\infty$)**：
    *   当上下文长度 $K \to \infty$ 时，熵趋于一个稳定值。
    *   关系式：$H_\infty \le ... \le H_2 \le H_1 \le H_0$。
    *   这证明增加语言成分（上下文）不会增加熵，反而会减少不定度。

## 三、 多余度（Redundancy）

### 1. 定义与计算
*   **多余度/羡余度 ($R$)**：反映语言结构的约束强弱。
*   **公式**：$R = 1 - \frac{H_\infty}{H_0}$
*   **案例（俄语）**：
    *   等概率熵 ($H_0$)：约 5.0
    *   极限熵 ($H_\infty$)：约 1.0
    *   多余度：$1 - 1/5 = 0.8 (80\%)$。
    *   意义：理想编码下，俄语文本可以压缩80%。

### 2. 多余度的四种类型
1.  **语法结构规定**：书面语中，语法限制了后续字母的可能性。
2.  **上下文推测**：口语或模糊文本中，根据上下文补全缺失信息（如“关于日报”推测为“光明日报”）。
3.  **字形特征**：文字笔画的冗余，遮挡一部分仍可识别（如俄文字母M）。
4.  **语音特征**：语音信号中的宽带属性，即使有噪声也能辨识。

### 3. 多余度的作用
*   **优点**：抗干扰，利于交际（纠错）。
*   **缺点**：在信道传输中造成负担。
*   **应用**：
    *   **压缩**：消除第4类多余度，更经济地利用信道。
    *   **编码**：设计最佳编码。

## 四、 语言熵的实证研究（Empirical Studies）

### 1. 印欧语系数据（香农等人的研究）
*   **英语**：字母熵约为 **4.03** bits。
*   **法语**：3.98 bits。
*   **德语**：4.10 bits。
*   **俄语**：4.35 bits（因有32个字母）。

### 2. 汉字熵的研究（老师的亲身经历与案例）
*   **1976年手工统计案例**：
    *   **背景**：文革期间，老师组织约10人，耗时数年。
    *   **数据量**：统计了500多万字的文本，包含12370个不同汉字。
    *   **结果**：计算出汉字熵为 **9.65** bits（假设汉字间无关联的$H_0$）。
    *   **验证**：后来北航刘源教授用计算机统计结果为 **9.71** bits，证明当年手工计算非常精准。
*   **古汉语**：
    *   字头更多（约15000+），熵约为 **10** bits。
*   **汉字多余度**：
    *   范围：约 0.56 - 0.74。
    *   **对比**：汉字的多余度比英语小（English redundancy is higher）。这意味着汉字文本比英语文本更“紧凑”，但利用上下文纠错的余地相对较小（相比英语）。

## 五、 最大熵模型的应用方向

### 1. 模型的别名与性质
*   **别名**：多元逻辑回归 (Multinomial Logistic Regression)。
*   **性质**：对数线性分类器 (Log-linear Classifier)。

### 2. 两种分类场景
1.  **非序列分类 (Analysis Classification)**：
    *   不考虑顺序，直接将文本/词归类。
    *   应用：文本分类（工业/农业/体育）、情感分析（褒义/贬义）。
2.  **序列分类 (Sequential Classification)**：
    *   考虑顺序，对句子中的每个词进行标注。
    *   应用：词性标注（Part-of-Speech Tagging）。
    *   **进阶模型**：将最大熵与马尔可夫模型结合，称为 **MEMM (Maximum Entropy Markov Model)**。

---
**核心观点总结**：
香农和马尔可夫从数学角度发现了语言文字中隐藏的规律（熵和状态转移），奠定了现代信息技术的基础。计算语言学的任务就是利用这些统计规律（如最大熵模型），通过特征提取和权重计算，实现对语言现象的自动分类和理解。


以下是根据录音文稿（文件57-58）整理的计算语言学课程知识要点笔记。本部分主要讲解了**最大熵模型的基本公式**，并深入探讨了作为基础的**线性回归**和**逻辑回归**。

# 计算语言学课程笔记（五十七、五十八）

## 一、 最大熵模型（Maximum Entropy）的基本公式

老师首先给出了最大熵分类模型的核心数学表达。

### 1. 基本公式
计算某个对象 $x$ 归属于类别 $c$ 的概率 $P(c|x)$：

$$ P(c|x) = \frac{1}{Z} \cdot e^{\sum_{i} w_i f_i(x,c)} $$

*   **$e$**：自然对数的底（约等于 2.71828）。
*   **指数部分**：由特征（Feature, $f_i$）与权重（Weight, $w_i$）的乘积之和构成。
    *   $f_i$：特征，例如单词是否以"ing"结尾。
    *   $w_i$：该特征对应的权重。
*   **$Z$**：规范化因子（Normalization factor），用于确保概率之和为1。$Z$ 等于所有可能类别的 $w$ 与 $f$ 乘积之和的指数的总和。

### 2. 公式书写习惯
在教科书中，为了书写方便，常将指数形式 $e^{...}$ 写为 `exp(...)`。

---

## 二、 回归（Regression）的概念与线性回归

为了深入理解最大熵模型，需要先理解“回归”。回归本质上是一种**分类**，但输出的是**实数值**（概率或程度），而非简单的“是/否”。

### 1. 分类 vs 回归
*   **分类 (Classification)**：将事物归入离散的类别（如：属于工业类、农业类）。输出通常是标签（Yes/No）。
*   **回归 (Regression)**：输出一个实数值（Real Value），表示属于某类的可能性或具体数值（如：0.33的可能性属于体育类）。

### 2. 线性回归 (Linear Regression) 案例：房地产价格预测
老师引用了《魔鬼经济学》（*Freakonomics*）中的房地产案例来解释线性回归。

*   **问题**：房地产广告中的词汇如何影响房屋最终售价？
*   **特征 (Features)**：
    *   **模糊形容词 (Vague adjectives)**：如 "fantastic"（好极了）、"cute"（逗人喜爱）、"charming"。使用这些词往往意味着房子有缺陷，导致售价降低（负权重）。
    *   **具体描述词**：如 "maple"（枫树）、"granite"（花岗岩）。使用这些词往往意味着售价较高（正权重）。
*   **线性方程模型**：
    房屋售价 ($Y$) 与模糊形容词数量 ($X$) 呈线性关系。
    $$ Y = w_0 + w_1 \cdot f_1 + w_2 \cdot f_2 + ... $$
    *   其中 $f_1$ 是模糊形容词数量，$w_1$ 是其权重（案例中为负数，如 -4900）。
    *   其他特征：抵押率、未售出房屋数量等。

### 3. 点积 (Dot Product) 表示法
为了简化公式，引入向量点积的概念。
*   **定义**：两个向量对应分量相乘并求和。
*   **应用**：线性回归公式可简化为权重向量 $W$ 和特征向量 $F$ 的点积。
    $$ Y = W \cdot F = \sum w_i f_i $$

### 4. 线性回归的训练
*   **目标**：找到一组权重 $W$，使得模型的**预测值** (Predicted) 与**观察值** (Observed) 之间的差距最小。
*   **代价函数 (Cost Function)**：使用**误差平方和**来衡量差距。
    $$ \Delta = \sum (Y_{predicted} - Y_{observed})^2 $$
    训练过程即求该代价函数的最小值。

---

## 三、 逻辑回归 (Logistic Regression)

线性回归输出的是 $(-\infty, +\infty)$ 的实数值，而分类概率需要在 $[0, 1]$ 之间。逻辑回归解决了这个问题。

### 1. 从线性回归到逻辑回归的推导
*   **目标**：将线性回归的结果 ($W \cdot F$) 映射到 $[0, 1]$ 区间，以表示概率。
*   **引入概念：优势率 (Odds)**
    *   优势率 = 事件发生的概率 / 事件不发生的概率
    *   $Odds = \frac{P(True)}{P(False)} = \frac{P(x)}{1 - P(x)}$
*   **分对数函数 (Logit Function)**
    *   对优势率取自然对数，令其等于线性回归的结果：
        $$ \ln(\frac{P}{1-P}) = W \cdot F $$
    *   这个过程即逻辑回归的核心：用线性函数来估计概率的对数优势。

### 2. 核心公式（逻辑函数/Sigmoid）
通过上述推导，可以反解出概率 $P$ 的计算公式（将值域限制在 0 到 1 之间）：

*   **情况A：属于“真” (True) 的概率**
    $$ P(True) = \frac{e^{W \cdot F}}{1 + e^{W \cdot F}} = \frac{1}{1 + e^{-W \cdot F}} $$
*   **情况B：属于“假” (False) 的概率**
    $$ P(False) = \frac{1}{1 + e^{W \cdot F}} = \frac{e^{-W \cdot F}}{1 + e^{-W \cdot F}} $$

*(注：原文中通过分子分母同乘 $e^{-W \cdot F}$ 进行了公式变形，说明了两种形式是等价的)*

### 3. 分类决策与超平面
*   **决策规则**：
    *   如果 $P(True) > P(False)$，则分类为“真”。
    *   这等价于判断优势率 $Odds > 1$。
    *   这进一步等价于判断线性部分 $W \cdot F > 0$。
*   **几何意义**：
    *   $W \cdot F = 0$ 构成了一个**超平面 (Hyperplane)**。
    *   逻辑回归通过判断样本点位于超平面的哪一侧（$W \cdot F$ 是正数还是负数）来进行分类。

### 4. 逻辑回归的训练
*   **目标**：选择权重 $W$，使得在训练集上观察到的结果出现的概率最大化。
*   **方法**：最大似然估计 (Maximum Likelihood Estimation)。即寻找参数 $W$，使得 $P(Y_{observed} | X)$ 最大。

以下是根据课堂录音文稿（第五十九、六十部分）整理的计算语言学课程知识要点笔记：

# 计算语言学（Computational Linguistics）课程笔记

## 一、 最大熵模型（Maximum Entropy Model）的数学构建

老师详细讲解了最大熵模型在分类问题中的数学表达和计算逻辑。

### 1. 核心公式推导
*   **目标**：计算在给定上下文 $X$ 的条件下，归类为类别 $C$（如名词、动词等）的概率 $P(C|X)$。
*   **基本形式**：这是一个基于**对数线性模型（Log-linear model）**的算法。
*   **概率计算公式**：
    $$P(c|x) = \frac{1}{Z(x)} \exp\left(\sum_{i} w_i f_i(x, c)\right)$$
    *   **$f_i(x, c)$ (Feature)**：特征函数，表示上下文 $x$ 和类别 $c$ 之间的某种特征关系。
    *   **$w_i$ (Weight)**：权值，对应每个特征的重要性（通过语料库统计训练得出）。
    *   **$\exp(\dots)$**：以 $e$ 为底的指数函数（录音中多次提到“以e为底的幂”）。
    *   **$Z(x)$ (Normalization Factor)**：归一化因子（录音中称为“Z”或分母部分）。

### 2. 归一化因子 $Z(x)$ 的含义
*   $Z(x)$ 是所有可能类别的“指数得分”之和。
*   **作用**：确保计算出的各类别概率之和为1。
*   **计算逻辑**：
    *   假设一个词 $X$ 可能属于 $N$ 个类别（如名词、动词、形容词）。
    *   **分子**：属于当前目标类别的特征与权值点积（Dot Product）的指数。
    *   **分母 ($Z$)**：将 $X$ 归为“名词”的指数得分 + 归为“动词”的指数得分 + 归为“形容词”的指数得分...（遍历所有可能的类 $C'$）。

### 3. 本质与联系
*   **逻辑回归（Logistic Regression）**：最大熵模型在分类上的应用本质上属于逻辑回归（Multinomial Logistic Regression）。
*   **最优化问题**：训练过程是求对数似然（Log-likelihood）的最大值，通常使用迭代算法（如改进的迭代尺度法 IIS 或 L-BFGS 等，录音中提到“专门的软件包”）。

---

## 二、 案例分析：词性标注（POS Tagging）

老师使用具体的语言学案例来演示如何利用特征和权值进行分类。

### 1. 案例背景
*   **句子**：*"Secretariat is expected to **race** tomorrow."*
*   **任务**：判断句子中的单词 **"race"** 的词性。
*   **候选类别**：
    *   $C_1$：名词 (NN)
    *   $C_2$：动词 (VB) - *注：录音中提及可能有形容词等其他情况，但此处主要对比这两类。*

### 2. 特征工程 (Feature Engineering)
老师“拍脑袋”设计了6个假设特征（实际应用中需从语料库统计），并赋予了假设的权值（Weight）：

| 特征编号 | 特征描述 (Feature Description) | 关联类别 | 假设权值 ($w$) |
| :--- | :--- | :--- | :--- |
| **$F_1$** | 当前词是 "race" (本身作为名词的可能性) | NN (名词) | 0.8 |
| **$F_2$** | 前一个词是 "to" | VB (动词) | 0.8 |
| **$F_3$** | 后缀是 "-ing" (当前词无此特征，故为0) | - | - |
| **$F_4$** | 当前词全是小写 (lower case) | VB (动词) | 0.01 (假设值) |
| **$F_5$** | 当前词是 "race" (本身作为动词的可能性) | VB (动词) | 0.1 |
| **$F_6$** | 前一个词是 "to" | NN (名词) | -1.3 (负值，表示极不可能) |

### 3. 计算过程演示
老师通过具体数值演示了 $P(NN|race)$ 和 $P(VB|race)$ 的计算：

1.  **计算名词 (NN) 的得分**：
    *   激活特征：$F_1$ (是race), $F_6$ (前接to)。
    *   指数部分：$\exp(w_1 \times 1 + w_6 \times 1) = \exp(0.8 + (-1.3)) = \exp(-0.5)$。
    *   *注：录音中口算数值可能有波动，核心逻辑是将相关特征权值相加后求指数。*

2.  **计算动词 (VB) 的得分**：
    *   激活特征：$F_2$ (前接to), $F_4$ (小写), $F_5$ (是race)。
    *   指数部分：$\exp(w_2 + w_4 + w_5) = \exp(0.8 + 0.01 + 0.1) = \exp(0.91)$。

3.  **计算归一化因子 $Z$**：
    *   $Z = \text{Score}(NN) + \text{Score}(VB)$。

4.  **得出概率**：
    *   $P(NN) = \text{Score}(NN) / Z$
    *   $P(VB) = \text{Score}(VB) / Z$
    *   **结果**：在此例中，动词的概率（如0.82）远高于名词（如0.18），因此系统判定为**动词**。

### 4. 特征选择的讨论
*   **有效特征**：如“前一个词是to”，对区分动词很有帮助。
*   **无效特征**：如“句首单词首字母大写”。
    *   *原因*：无论该词是专有名词还是普通词，放在句首都会大写，这个特征没有区分度（Discriminative power），对分类没有帮助。
*   **关键点**：最大熵模型的效果取决于能否找到**强有力的特征**以及**准确的权值**。

---

## 三、 理论基础：为什么要用最大熵？（The "Why"）

### 1. 历史背景
*   **提出者**：E.T. Jaynes (1957年)。
*   **最大熵原理 (Maximum Entropy Principle)**。

### 2. 核心思想
*   **定义**：熵 (Entropy) 代表随机变量的不确定性。熵越大，不确定性越大（分布越均匀）。
*   **推断原则**：在掌握了部分知识（约束条件）的前提下，对未知分布最合理的推断应该是：
    *   **符合已知知识**（满足所有特征约束）。
    *   **熵最大**（在满足约束的条件下，让不确定性最大化）。
*   **哲学解释**：
    *   这是**“最不偏不倚” (Unbiased)** 的选择。
    *   如果你不知道其他信息，就不要假设任何额外的约束或偏好。
    *   除了已知的事实（Feature constraints），其余部分应保持最混乱（随机/均匀）的状态。
    *   任何其他选择都意味着你引入了某种缺乏根据的假设（Bias）。

### 3. 简单例子
*   如果完全没有任何约束，对一个词的词性判断应该是均匀分布（概率相等）。
*   一旦引入特征（如“前面是to”），概率分布会向符合特征的方向倾斜，但未被特征描述的部分依然保持最大熵。

---

## 四、 总结
1.  **模型公式**：记忆 $P(c|x)$ 的指数形式公式，核心是特征与权值的乘积之和。
2.  **工作流程**：
    *   发现特征（语言学知识）。
    *   训练权值（语料库统计/算法迭代）。
    *   带入公式计算分类概率。
    *   选择概率最大的类别。
3.  **学科交叉**：虽然公式是数学的，但特征的挖掘离不开语言学知识（如形态学、句法结构等）。有效的特征决定了模型的上限。

这是根据课堂录音文稿（计算语言学 第六十一、六十二讲）整理的知识要点笔记。

# 计算语言学课堂笔记：最大熵模型与最大熵马尔可夫模型

## 一、 最大熵模型 (Maximum Entropy Model) 的构建原理

### 1. 概念引入：如何处理未知的概率分布
*   **场景假设**：对一个未知词进行词性标注（POS Tagging），词类标记集共有45个。
*   **完全无知的情况（最大熵状态）**：
    *   当我们对该词一无所知时，最合理（最自然）的假设是它属于这45类的概率是均等的。
    *   概率 $P = 1/45$。
    *   此时**熵（Entropy）最大**，意味着不确定性最高，不对未知做任何预设。
*   **加入部分知识（约束条件）**：
    *   假设根据训练数据，知道该词只可能是四种类别之一（如：NN, JJ, NNS, V）。
    *   在没有更多信息时，这四类的概率均分为 $1/4$，其他类别为0。
*   **非等概率分布**：
    *   如果进一步知道它作为名词（NN）的可能性更高（如1/2），其他三类平分剩余概率。
    *   **计算方法**：利用熵的公式 $H(X) = - \sum P(x) \log P(x)$ 来衡量分布的合理性。

### 2. 建模原则：奥卡姆剃刀 (Occam's Razor)
*   **原则内容**：Plurality should never be posited without necessity.（如无必要，勿增实体）。
*   **在MaxEnt中的应用**：
    *   在建立模型时，只加入有根据的约束（Constraint/Feature）。
    *   没有证据支持的事情不要随便假设。
    *   在满足所有已知约束条件的前提下，选择**熵最大**（即最均匀、最不确定）的模型分布。

### 3. 特征选择与概率调整案例
*   **案例：单词 "Take" 的机器翻译歧义**
    *   **多义性**：Take 有7种可能的翻译（抓、拿走、乘坐、吃药、花费、理解等）。
    *   **初始状态**：最大熵假设下，每种翻译概率为 $1/7$。
    *   **引入特征（Feature）进行约束**：
        1.  **统计特征**：观察发现前两种翻译最常见，概率和为 $2/5$（各 $1/5$），剩余5种平分 $3/5$。
        2.  **上下文特征**：
            *   如果上下文出现 "bus"（indicator function 指示函数 $f(x,y)=1$），则输出“乘坐”。
            *   引入此特征后，模型会将“乘坐”的概率推高，其他概率降低。
    *   **结论**：通过不断增加经过精心选择的特征（Feature），在保持其余部分熵最大的同时，消除不确定性。

### 4. 模型的数学定义（IBM观点）
*   最大熵模型的最优化问题：从允许的概率分布集合 $C$ 中，选择一个熵 $H(p)$ 最大的模型。
*   这等价于**多项逻辑回归（Multinomial Logistic Regression）**或**对数线性模型（Log-Linear Model）**。
*   通过训练权值（Weights, $W$），使训练数据的似然度（Likelihood）最大化，同时保持熵最大。

---

## 二、 从最大熵到序列标注：最大熵马尔可夫模型 (MEMM)

### 1. 为什么要提出 MEMM？
*   **基本最大熵模型的局限**：
    *   它是**分类模型（Classification）**，适用于文本分类、垃圾邮件识别等。
    *   它处理的是单点决策，**不考虑序列顺序**（Sequence）。
*   **隐马尔可夫模型 (HMM) 的局限**：
    *   HMM 考虑了顺序（转移概率），但**特征极其有限**。
    *   HMM 只有两个特征来源：
        1.  **转移概率 (Transition Probability)**：Tag 到 Tag。
        2.  **发射概率 (Emission Probability)**：Tag 到 Word。
    *   无法利用丰富的上下文特征（如：首字母大写、后缀是-ed/-ing、前一个词是什么、后一个词是什么等）。
*   **解决方案**：结合 MaxEnt 的多特征能力与 HMM 的序列能力 $\rightarrow$ **MEMM**。

### 2. HMM 与 MEMM 的核心区别（理论对比）

| 比较维度 | 隐马尔可夫模型 (HMM) | 最大熵马尔可夫模型 (MEMM) |
| :--- | :--- | :--- |
| **模型类型** | **生成模型 (Generative Model)** | **判别模型 (Discriminative Model)** |
| **计算目标** | 先验概率 (Prior) * 发射概率 | **后验概率 (Posterior Probability)** |
| **核心公式** | $P(Tag_{i}|Tag_{i-1}) \times P(Word_{i}|Tag_{i})$ | $P(Tag_{i} \| Word_{i}, Tag_{i-1}, Features)$ |
| **特征使用** | 仅依赖转移和发射概率，很难加其他特征 | 可以加入任意特征（大小写、形态、上下文单词等） |
| **决策依据** | 联合概率最大化 | 条件概率最大化 |

### 3. MEMM 的工作机制
*   **计算逻辑**：
    *   不再计算“这个Tag生成这个词的概率”，而是直接计算“在当前词、前一个Tag以及各种特征条件下，当前Tag是某状态的概率”。
    *   **特征函数**：可以使用 $f(Context, Tag)$，例如：
        *   特征1：前一个Tag是 $NP$，当前词首字母大写 $\rightarrow$ 当前Tag可能是 $NNP$。
        *   特征2：当前词以 "ly" 结尾 $\rightarrow$ 当前Tag可能是 $RB$。
*   **优势**：能够融合语言学知识（形态学、句法环境等）作为特征，比单纯依赖统计的 HMM 更灵活、效果通常更好。

---

## 三、 MEMM 的解码与训练 (Viterbi 算法的变体)

### 1. 维特比算法 (Viterbi Algorithm) 在 MEMM 中的应用
老师通过“吃冰淇淋（冷/热天气推断）”的案例演示了计算过程。

*   **HMM 的 Viterbi 路径计算**：
    $$V_{t}(j) = \max_{i} [ V_{t-1}(i) \times P(Tag_j|Tag_i) \times P(Word_t|Tag_j) ]$$
    (前一步累积概率 $\times$ 转移概率 $\times$ 发射概率)

*   **MEMM 的 Viterbi 路径计算**：
    $$V_{t}(j) = \max_{i} [ V_{t-1}(i) \times P(Tag_j \| Context, Tag_i) ]$$
    *   **关键点**：用 MaxEnt 算出的**后验概率** $P(Tag_j | \dots)$ 直接替换了 HMM 中的 (转移概率 $\times$ 发射概率)。
    *   即：利用最大熵模型，根据当前观察值 $O$ 和前一状态 $S_{prev}$ 及其他特征，计算跳转到当前状态 $S_{curr}$ 的概率。

### 2. 训练 (Training)
*   可以使用有指导的训练（Supervised Training）。
*   利用已标注的语料库（Tagging Corpus），提取特征，训练最大熵模型的权值（Weights）。
*   目标：使训练语料的对数似然度（Log-Likelihood）最大化。
*   算法：GIS (Generalized Iterative Scaling), IIS (Improved Iterative Scaling) 或梯度上升等（文中提及“老式的算法”）。

---

## 四、 老师的总结与教学理念
*   **理论的重要性**：
    *   很多学生或研究者只会调用工具包（Toolkits），却不知道背后的原理（如只知道用MaxEnt工具，不知何为熵）。
    *   老师编写教材和讲课的目的是为了**讲清原理**（Why it works），而不仅仅是操作步骤。
    *   理解原理（如判别模型 vs 生成模型、后验概率 vs 先验概率）对于深入研究至关重要。
*   **学习建议**：
    *   不能“糊里糊涂”地使用别人的东西。
    *   鼓励学生多看书，甚至参考国外优秀的讲义（PPT）来互证，把理论彻底吃透。
    *   MaxEnt 和 HMM 是计算语言学中应用最多、讨论最多但也最容易被误解的模型。

## 五、 专有名词对照
*   Maximum Entropy (MaxEnt): 最大熵
*   Constraint: 约束
*   Plurality should never be posited without necessity: 如无必要，勿增实体（奥卡姆剃刀）
*   Indicator Function: 指示函数
*   Discriminative Model: 分辨模型/判别模型
*   Generative Model: 生成模型
*   Posterior Probability: 后验概率
*   Prior Probability: 先验概率
*   Transition Probability: 转移概率
*   Emission Probability: 发射概率
*   Hidden Markov Model (HMM): 隐马尔可夫模型
*   Maximum Entropy Markov Model (MEMM): 最大熵马尔可夫模型
*   Viterbi Algorithm: 维特比算法
*   Logistic Regression: 逻辑回归